{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from pclib.nn.models import ConvClassifierInv, ConvClassifier\n",
    "from pclib.nn.layers import Conv2d\n",
    "from pclib.optim.train_conv import train_conv as train\n",
    "from pclib.utils.plot import plot_stats\n",
    "from pclib.optim.eval import track_vfe, accuracy\n",
    "from pclib.utils.functional import format_y\n",
    "from pclib.utils.customdataset import PreloadedDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for Model:\n\tsize mismatch for stats.output: copying a param with shape torch.Size([3]) from checkpoint, the shape in current model is torch.Size([0]).",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[57], line 29\u001b[0m\n\u001b[0;32m     26\u001b[0m state_dict \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mstate_dict()\n\u001b[0;32m     28\u001b[0m model \u001b[38;5;241m=\u001b[39m Model()\n\u001b[1;32m---> 29\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate_dict\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     30\u001b[0m model\u001b[38;5;241m.\u001b[39mstats\u001b[38;5;241m.\u001b[39moutput\n",
      "File \u001b[1;32mc:\\Users\\joeag\\Documents\\venvs\\ml-env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2041\u001b[0m, in \u001b[0;36mModule.load_state_dict\u001b[1;34m(self, state_dict, strict)\u001b[0m\n\u001b[0;32m   2036\u001b[0m         error_msgs\u001b[38;5;241m.\u001b[39minsert(\n\u001b[0;32m   2037\u001b[0m             \u001b[38;5;241m0\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMissing key(s) in state_dict: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m   2038\u001b[0m                 \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(k) \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m missing_keys)))\n\u001b[0;32m   2040\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(error_msgs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m-> 2041\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mError(s) in loading state_dict for \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m   2042\u001b[0m                        \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(error_msgs)))\n\u001b[0;32m   2043\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for Model:\n\tsize mismatch for stats.output: copying a param with shape torch.Size([3]) from checkpoint, the shape in current model is torch.Size([0])."
     ]
    }
   ],
   "source": [
    "class Stats(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.register_buffer('output', torch.zeros(0))\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.stats = Stats()\n",
    "\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(10, 1),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.layers(x) + torch.randn_like(x)\n",
    "        self.stats.output = torch.cat([self.stats.output, out.mean().view(1)])\n",
    "        return self.layers(x)\n",
    "\n",
    "model = Model()\n",
    "x = torch.randn(1, 10)\n",
    "model(x)\n",
    "model(x)\n",
    "model(x)\n",
    "state_dict = model.state_dict()\n",
    "\n",
    "model = Model()\n",
    "model.load_state_dict(state_dict)\n",
    "model.stats.output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def connectivity_matrix(in_shape, kernel_size, stride, padding):\n",
    "    \"\"\"\n",
    "    Returns a connectivity matrix for a convolutional layer.\n",
    "    \"\"\"\n",
    "    in_h, in_w = in_shape\n",
    "    # out_h, out_w = out_shape\n",
    "    out_h, out_w = (in_h + 2*padding[0] - kernel_size[0])//stride[0] + 1, (in_w + 2*padding[1] - kernel_size[1])//stride[1] + 1\n",
    "    k_h, k_w = kernel_size\n",
    "    s_h, s_w = stride\n",
    "    p_h, p_w = padding\n",
    "    \n",
    "    # Initialize connectivity matrix\n",
    "    C = torch.zeros(out_h*out_w, in_h*in_w)\n",
    "    \n",
    "    # Iterate over output pixels\n",
    "    for i in range(out_h):\n",
    "        for j in range(out_w):\n",
    "            # Iterate over kernel\n",
    "            for k in range(k_h):\n",
    "                for l in range(k_w):\n",
    "                    # Input pixel index\n",
    "                    ii = i*s_h + k - p_h\n",
    "                    jj = j*s_w + l - p_w\n",
    "                    \n",
    "                    # If input pixel is within bounds\n",
    "                    if ii >= 0 and ii < in_h and jj >= 0 and jj < in_w:\n",
    "                        # Input pixel index\n",
    "                        in_idx = ii*in_w + jj\n",
    "                        # Output pixel index\n",
    "                        out_idx = i*out_w + j\n",
    "                        # Set connectivity\n",
    "                        C[out_idx, in_idx] = 1\n",
    "    return C.t()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "150 150\n"
     ]
    }
   ],
   "source": [
    "# torch.manual_seed(42)\n",
    "in_shape = (1, 300, 300)\n",
    "kernel_size = 2\n",
    "stride = 2\n",
    "padding = 0\n",
    "out_channels = 1\n",
    "out_h, out_w = (in_shape[1] + 2*padding - kernel_size)//stride + 1, (in_shape[2] + 2*padding - kernel_size)//stride + 1\n",
    "print(out_h, out_w)\n",
    "\n",
    "\n",
    "layer = nn.Conv2d(\n",
    "    in_shape[0], \n",
    "    out_channels, \n",
    "    kernel_size,\n",
    "    stride,\n",
    "    padding, \n",
    "    bias=False\n",
    ")\n",
    "\n",
    "mat = connectivity_matrix((in_shape[1], in_shape[2]), (kernel_size, kernel_size), (stride,stride), (padding,padding))\n",
    "\n",
    "def run(mode='manual'):\n",
    "    assert mode in ['manual', 'auto'], 'Mode must be manual or auto'\n",
    "    if mode == 'auto':\n",
    "        x = torch.randn(in_shape).unsqueeze(0)\n",
    "        true = torch.randn((out_h, out_w)).unsqueeze(0)\n",
    "\n",
    "        pred = layer(x)\n",
    "\n",
    "        error = (true - pred)\n",
    "        error.pow(2).mean().backward()\n",
    "    elif mode == 'manual':\n",
    "        with torch.no_grad():\n",
    "            x = torch.randn(in_shape).unsqueeze(0)\n",
    "            true = torch.randn((out_h, out_w)).unsqueeze(0)\n",
    "\n",
    "            pred = layer(x)\n",
    "\n",
    "            error = (true - pred)\n",
    "            outer = -2 * torch.outer(x.flatten(), error.flatten()) * mat\n",
    "            # nonzeros = outer.nonzero(as_tuple=False)\n",
    "            # sorted_nonzeros = sorted(nonzeros, key=lambda x: x[1])\n",
    "            # vals = [outer[i,j] for i,j in sorted_nonzeros]\n",
    "            # vals_tight = torch.stack([torch.tensor(vals[i::kernel_size**2]) for i in range(kernel_size**2)])\n",
    "            # grad = vals_tight.mean(dim=1).reshape(layer.weight.shape)\n",
    "            grad = outer\n",
    "\n",
    "    # torch.isclose(manual_grad, layer.weight.grad).all()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([[[[-1.3307, -6.5906, -0.6320],\n",
       "           [-4.0812, -6.6632,  0.5901],\n",
       "           [ 3.3621, -4.8324, -6.9174]]],\n",
       " \n",
       " \n",
       "         [[[-1.0896,  2.0920, -1.8908],\n",
       "           [-3.1428,  5.5075, -0.8493],\n",
       "           [ 1.5190,  1.8407,  2.8830]]],\n",
       " \n",
       " \n",
       "         [[[-0.4862, -1.5449,  1.2864],\n",
       "           [-5.9475,  0.1644,  2.7083],\n",
       "           [ 3.4047, -2.1701, -3.0102]]]]),\n",
       " tensor([[[[-1.3307, -6.5906, -0.6320],\n",
       "           [-4.0812, -6.6632,  0.5901],\n",
       "           [ 3.3621, -4.8324, -6.9174]]],\n",
       " \n",
       " \n",
       "         [[[-1.0896,  2.0920, -1.8908],\n",
       "           [-3.1428,  5.5075, -0.8493],\n",
       "           [ 1.5190,  1.8407,  2.8830]]],\n",
       " \n",
       " \n",
       "         [[[-0.4862, -1.5449,  1.2864],\n",
       "           [-5.9475,  0.1644,  2.7083],\n",
       "           [ 3.4047, -2.1701, -3.0102]]]], grad_fn=<MulBackward0>),\n",
       " tensor([[[[ 11.0869,  11.0813],\n",
       "           [-10.2167,  16.1286]]]]),\n",
       " tensor([[[[ 11.0869,  11.0813],\n",
       "           [-10.2167,  16.1286]]]], grad_fn=<MulBackward0>),\n",
       " tensor([-5.9108]),\n",
       " tensor([-5.9108], grad_fn=<MulBackward0>))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "in_shape = (3, 1, 3, 3)\n",
    "kernel_size = 2\n",
    "stride = 1\n",
    "padding = 0\n",
    "out_channels = 1\n",
    "out_dim = (in_shape[2] + 2*padding - kernel_size)//stride + 1\n",
    "print(out_dim)\n",
    "\n",
    "\n",
    "layer = nn.Conv2d(\n",
    "    in_shape[1], \n",
    "    out_channels, \n",
    "    kernel_size,\n",
    "    stride,\n",
    "    padding, \n",
    "    bias=True\n",
    ")\n",
    "\n",
    "x1 = torch.randn((3, 1, out_dim, out_dim))\n",
    "x2 = torch.randn(in_shape, requires_grad=True)\n",
    "\n",
    "x1_hat = layer(x2)\n",
    "x2_hat = torch.randn_like(x2)\n",
    "# x2_hat = x2\n",
    "\n",
    "e1 = x1 - x1_hat\n",
    "e2 = x2 - x2_hat\n",
    "\n",
    "# e1 = torch.zeros_like(e1)\n",
    "\n",
    "vfe = e1.square().sum() + e2.square().sum()\n",
    "vfe.backward(retain_graph=True)\n",
    "\n",
    "# e2.square().sum().backward(retain_graph=True)\n",
    "\n",
    "x_manualgrad = torch.nn.grad.conv2d_input(x2.shape, layer.weight, e1, stride=stride, padding=padding)\n",
    "x_manualgrad = -2*(x_manualgrad - e2)\n",
    "w_grad = -2 * torch.nn.grad.conv2d_weight(x2, layer.weight.shape, e1, stride=stride, padding=padding)\n",
    "b_grad = -2 * e1.sum(dim=(0,2,3))\n",
    "\n",
    "# torch.isclose(x_autograd, x_manualgrad)#, torch.isclose(w_grad, layer.weight.grad).all()\n",
    "x2.grad, x_manualgrad, layer.weight.grad, w_grad, layer.bias.grad, b_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "could not create a descriptor for a dilated convolution forward propagation primitive",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[141], line 39\u001b[0m\n\u001b[0;32m     35\u001b[0m vfe\u001b[38;5;241m.\u001b[39mbackward(retain_graph\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     37\u001b[0m \u001b[38;5;66;03m# e2.square().sum().backward(retain_graph=True)\u001b[39;00m\n\u001b[1;32m---> 39\u001b[0m x_manualgrad \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgrad\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2d_input\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlayer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43me1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstride\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     40\u001b[0m x_manualgrad \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m\u001b[38;5;241m*\u001b[39m(x_manualgrad \u001b[38;5;241m-\u001b[39m e2)\n\u001b[0;32m     41\u001b[0m w_grad \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m \u001b[38;5;241m*\u001b[39m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mgrad\u001b[38;5;241m.\u001b[39mconv2d_weight(x2, layer\u001b[38;5;241m.\u001b[39mweight\u001b[38;5;241m.\u001b[39mshape, e1, stride\u001b[38;5;241m=\u001b[39mstride, padding\u001b[38;5;241m=\u001b[39mpadding)\n",
      "File \u001b[1;32mc:\\Users\\joeag\\Documents\\venvs\\ml-env\\Lib\\site-packages\\torch\\nn\\grad.py:97\u001b[0m, in \u001b[0;36mconv2d_input\u001b[1;34m(input_size, weight, grad_output, stride, padding, dilation, groups)\u001b[0m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     72\u001b[0m \u001b[38;5;124;03mComputes the gradient of conv2d with respect to the input of the convolution.\u001b[39;00m\n\u001b[0;32m     73\u001b[0m \u001b[38;5;124;03mThis is same as the 2D transposed convolution operator under the hood but requires\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     93\u001b[0m \n\u001b[0;32m     94\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     95\u001b[0m \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m grad_output\u001b[38;5;241m.\u001b[39mnew_empty(\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mexpand(input_size)\n\u001b[1;32m---> 97\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maten\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvolution_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgrad_output\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     98\u001b[0m \u001b[43m                                           \u001b[49m\u001b[43m_pair\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstride\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_pair\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpadding\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_pair\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdilation\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     99\u001b[0m \u001b[43m                                           \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgroups\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\joeag\\Documents\\venvs\\ml-env\\Lib\\site-packages\\torch\\_ops.py:502\u001b[0m, in \u001b[0;36mOpOverloadPacket.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    497\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    498\u001b[0m     \u001b[38;5;66;03m# overloading __call__ to ensure torch.ops.foo.bar()\u001b[39;00m\n\u001b[0;32m    499\u001b[0m     \u001b[38;5;66;03m# is still callable from JIT\u001b[39;00m\n\u001b[0;32m    500\u001b[0m     \u001b[38;5;66;03m# We save the function ptr as the `op` attribute on\u001b[39;00m\n\u001b[0;32m    501\u001b[0m     \u001b[38;5;66;03m# OpOverloadPacket to access it here.\u001b[39;00m\n\u001b[1;32m--> 502\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_op\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: could not create a descriptor for a dilated convolution forward propagation primitive"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "batch_size = 3\n",
    "in_channels = 1\n",
    "in_dim = 3\n",
    "kernel_size = 2\n",
    "stride = 1\n",
    "padding = 0\n",
    "out_channels = 1\n",
    "out_dim = (in_shape[2] + 2*padding - kernel_size)//stride + 1\n",
    "print(out_dim)\n",
    "\n",
    "\n",
    "layer = nn.ConvTranspose2d(\n",
    "    in_channels,\n",
    "    out_channels, \n",
    "    kernel_size,\n",
    "    stride,\n",
    "    padding, \n",
    "    bias=True\n",
    ")\n",
    "\n",
    "x1 = torch.randn((batch_size, in_channels, in_dim, in_dim))\n",
    "x2 = torch.randn((batch_size, out_channels, out_dim, out_dim), requires_grad=True)\n",
    "\n",
    "x1_hat = layer(x2)\n",
    "x2_hat = torch.randn_like(x2)\n",
    "# x2_hat = x2\n",
    "\n",
    "e1 = x1 - x1_hat\n",
    "e2 = x2 - x2_hat\n",
    "\n",
    "# e1 = torch.zeros_like(e1)\n",
    "\n",
    "vfe = e1.square().sum() + e2.square().sum()\n",
    "vfe.backward(retain_graph=True)\n",
    "\n",
    "# e2.square().sum().backward(retain_graph=True)\n",
    "\n",
    "x_manualgrad = torch.nn.grad.conv2d_input(x2.shape, layer.weight, e1, stride=stride, padding=padding)\n",
    "x_manualgrad = -2*(x_manualgrad - e2)\n",
    "w_grad = -2 * torch.nn.grad.conv2d_weight(x2, layer.weight.shape, e1, stride=stride, padding=padding)\n",
    "b_grad = -2 * e1.sum(dim=(0,2,3))\n",
    "\n",
    "# torch.isclose(x_autograd, x_manualgrad)#, torch.isclose(w_grad, layer.weight.grad).all()\n",
    "x2.grad, x_manualgrad, layer.weight.grad, w_grad, layer.bias.grad, b_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[-1.1280, -0.4667],\n",
      "          [-0.2554, -0.1569]],\n",
      "\n",
      "         [[-1.0597, -1.1517],\n",
      "          [-1.4969, -1.2904]]]]) \n",
      " tensor([[[[1.3864, 0.3418],\n",
      "          [0.6081, 1.2601]],\n",
      "\n",
      "         [[0.8293, 1.1670],\n",
      "          [1.1651, 1.0561]]]], grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "batch_size = 1\n",
    "C0, H0, W0 = 1, 4, 4\n",
    "C1, H1, W1 = 2, 2, 2\n",
    "kernel_size = 3\n",
    "stride = 1\n",
    "padding = 1\n",
    "upsample = 2\n",
    "\n",
    "downsample = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "layer = Conv2d(\n",
    "    (C1, H1, W1),\n",
    "    C0, \n",
    "    kernel_size,\n",
    "    stride,\n",
    "    padding, \n",
    "    upsample=upsample,\n",
    "    has_bias=True\n",
    ")\n",
    "\n",
    "s0 = {\n",
    "    'x': torch.randn((batch_size, C0, H0, W0)),\n",
    "    'e': torch.randn((batch_size, C0, H0, W0))\n",
    "}\n",
    "s1 = {\n",
    "    'x': torch.randn((batch_size, C1, H1, W1), requires_grad=True),\n",
    "    'e': torch.randn((batch_size, C1, H1, W1))\n",
    "}\n",
    "\n",
    "s0['pred'] = layer.predict(s1)\n",
    "s1['pred'] = torch.randn_like(s1['x'])\n",
    "s1['pred'] = s1['x'].clone()\n",
    "\n",
    "\n",
    "s0['e'] = s0['x'] - s0['pred']\n",
    "s1['e'] = s1['x'] - s1['pred']\n",
    "\n",
    "vfe = s0['e'].square().sum() + s1['e'].square().sum()\n",
    "# vfe = s0['e'].square().sum()\n",
    "# vfe = s1['e'].square().sum()\n",
    "\n",
    "vfe.backward(retain_graph=True)\n",
    "\n",
    "# print(downsample(s0['e']).shape)\n",
    "\n",
    "s1x_grad = torch.nn.grad.conv2d_input(s1['x'].shape, layer.conv_td[0].weight, downsample(s0['e']), stride=stride, padding=padding)\n",
    "print(s1['x'].grad, \"\\n\", 2*s1x_grad)\n",
    "\n",
    "# x_manualgrad = torch.nn.grad.conv2d_input(x2.shape, layer.weight, e1, stride=stride, padding=padding)\n",
    "# x_manualgrad = -2*(x_manualgrad - e2)\n",
    "# w_grad = -2 * torch.nn.grad.conv2d_weight(x2, layer.weight.shape, e1, stride=stride, padding=padding)\n",
    "# b_grad = -2 * e1.sum(dim=(0,2,3))\n",
    "\n",
    "# # torch.isclose(x_autograd, x_manualgrad)#, torch.isclose(w_grad, layer.weight.grad).all()\n",
    "# x2.grad, x_manualgrad, layer.weight.grad, w_grad, layer.bias.grad, b_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[[ 0.0366,  0.3688],\n",
       "           [-0.4128, -0.2120]]]]),\n",
       " tensor([[[[ 0.0366,  0.0366,  0.3688,  0.3688],\n",
       "           [ 0.0366,  0.0366,  0.3688,  0.3688],\n",
       "           [-0.4128, -0.4128, -0.2120, -0.2120],\n",
       "           [-0.4128, -0.4128, -0.2120, -0.2120]]]]),\n",
       " tensor([[[[ 0.0366,  0.3688],\n",
       "           [-0.4128, -0.2120]]]]))"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "upsample = nn.Upsample(scale_factor=2, mode='nearest')\n",
    "downsample = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "x = torch.randn((1, 1, 2, 2))\n",
    "y = upsample(x)\n",
    "x_hat = downsample(y)\n",
    "x, y, x_hat"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
