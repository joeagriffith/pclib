reTanh is the only acceptable activation function:
- SSLI achieves 96% +- 1% val accuracy
- just tanh also does very well (97%). Need to compare empirically.
- the network fails to learn using the sigmoid function
- 


TO TRY:
Different Inhibition patterns
- specifically, a Thick diagonal, as opposed to step-wise blocks. THis means no two nodes have the same Inhibition-area.

Implement Boosting
- Some features may end up never firing due to poor initialisation.
- a dynamic boosting of sparsely activating features during learning may help the model learn more effective features.