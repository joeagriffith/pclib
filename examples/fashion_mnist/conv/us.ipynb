{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "from pclib.nn.models import ConvClassifierUs\n",
    "from pclib.nn.layers import Conv2d\n",
    "from pclib.optim.train import train\n",
    "from pclib.optim.eval import track_vfe, accuracy\n",
    "from pclib.utils.functional import format_y, identity\n",
    "from pclib.utils.customdataset import PreloadedDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seed = 42\n",
    "torch.use_deterministic_algorithms(True)\n",
    "os.environ['CUBLAS_WORKSPACE_CONFIG'] = ':4096:8'\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/48000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                        \r"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(seed)\n",
    "class TanhTransform(object):\n",
    "    def __init__(self, a=1., b=0., c=1.0):\n",
    "        self.a = a\n",
    "        self.b = b\n",
    "        self.c = c\n",
    "\n",
    "    def __call__(self, img):\n",
    "        return ((img * self.a).tanh() + self.b) * self.c\n",
    "\n",
    "class PadTransform(object):\n",
    "    def __call__(self, img):\n",
    "        return F.pad(img, (2, 2, 2, 2), mode='constant', value=0)\n",
    "\n",
    "class InvTanhTransform(object):\n",
    "    def __call__(self, img):\n",
    "        num = 1 + img\n",
    "        div = (1 - img).clamp(min=1e-6)\n",
    "        m = 0.5 * torch.log(num / div)\n",
    "        return m\n",
    "\n",
    "class SigmoidTransform(object):\n",
    "    def __call__(self, img):\n",
    "        return img.sigmoid()\n",
    "    \n",
    "class ReLUTanhTransform(object):\n",
    "    def __call__(self, img):\n",
    "        return F.relu(img.tanh())\n",
    "\n",
    "class AddTransform(object):\n",
    "    def __init__(self, add):\n",
    "        self.add = add\n",
    "\n",
    "    def __call__(self, img):\n",
    "        return img + self.add\n",
    "\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    PadTransform(),\n",
    "    # transforms.RandomAffine(degrees=10, translate=(0.1, 0.1), scale=(0.9, 1.1)),                                \n",
    "    # AddTransform(0.4242),\n",
    "])\n",
    "\n",
    "val_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    PadTransform(),\n",
    "    # InvTanhTransform(),\n",
    "    # AddTransform(0.4242),\n",
    "    # SigmoidTransform(),\n",
    "    # TanhTransform(a=2.0),\n",
    "    # TanhTransform(a=1.0, b=1.0, c=0.5),\n",
    "])\n",
    "\n",
    "dataset = datasets.FashionMNIST('../Datasets/', train=True, download=True, transform=transforms.ToTensor())\n",
    "# dataset.data = dataset.data / 255.0\n",
    "# # shorten dataset\n",
    "# length = 1000\n",
    "# dataset = torch.utils.data.Subset(dataset, range(length))\n",
    "\n",
    "VAL_RATIO = 0.2\n",
    "val_len = int(len(dataset) * VAL_RATIO)\n",
    "train_len = len(dataset) - val_len\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_len, val_len])\n",
    "train_dataset = PreloadedDataset.from_dataset(train_dataset, train_transform, device)\n",
    "val_dataset = PreloadedDataset.from_dataset(val_dataset, val_transform, device)\n",
    "INPUT_SHAPE = 784\n",
    "NUM_CLASSES = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1500x500 with 10 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max value:  tensor(1., device='cuda:0')\n",
      "Min value:  tensor(0., device='cuda:0')\n",
      "Mean:  tensor(0.2187, device='cuda:0')\n",
      "Std:  tensor(0.3316, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "# Show some images\n",
    "fig, axes = plt.subplots(1, 10, figsize=(15, 5))\n",
    "for i, ax in enumerate(axes):\n",
    "    img, label = train_dataset[i]\n",
    "    ax.imshow(img.squeeze().cpu(), cmap='gray')\n",
    "    ax.axis('off')\n",
    "plt.show()\n",
    "\n",
    "# print max min values\n",
    "print('Max value: ', train_dataset.transformed_images.max())\n",
    "print('Min value: ', train_dataset.transformed_images.min())\n",
    "print('Mean: ', train_dataset.transformed_images.mean())\n",
    "print('Std: ', train_dataset.transformed_images.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(seed)\n",
    "\n",
    "model_name = 'ConvUsStrided256-tanh'\n",
    "model = ConvClassifierUs(\n",
    "    bias=True,\n",
    "    symmetric=True, \n",
    "    precision_weighted=False,\n",
    "    actv_fn=F.tanh,\n",
    "    steps=100,\n",
    "    gamma=0.1,\n",
    "    ).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [0/50]:   0%|          | 0/48 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00009: reducing learning rate of group 0 to 1.0000e-03.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00014: reducing learning rate of group 0 to 1.0000e-04.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                          \r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[38], line 7\u001b[0m\n\u001b[0;32m      4\u001b[0m log_dir \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mexamples/fashion_mnist/logs/conv/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m      6\u001b[0m NUM_EPOCHS \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m50\u001b[39m\n\u001b[1;32m----> 7\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mval_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mNUM_EPOCHS\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.001\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43mc_lr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.01\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mBATCH_SIZE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreg_coeff\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.02\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     16\u001b[0m \u001b[43m    \u001b[49m\u001b[43mflatten\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     17\u001b[0m \u001b[43m    \u001b[49m\u001b[43msave_best\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     18\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mAdamW\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     19\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mauto\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     20\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlog_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlog_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     21\u001b[0m \u001b[43m    \u001b[49m\u001b[43mminimal_stats\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     22\u001b[0m \u001b[43m    \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mReduceLROnPlateau\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     23\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;66;03m# NUM_EPOCHS = 20\u001b[39;00m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;66;03m# train(\u001b[39;00m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;66;03m#     model, \u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     41\u001b[0m \u001b[38;5;66;03m#     scheduler='ReduceLROnPlateau',\u001b[39;00m\n\u001b[0;32m     42\u001b[0m \u001b[38;5;66;03m# )\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\joeag\\Documents\\pclib\\pclib\\optim\\train.py:311\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(model, train_data, val_data, num_epochs, lr, c_lr, batch_size, reg_coeff, flatten, neg_coeff, untr_coeff, log_dir, minimal_stats, track_corr, assert_grads, val_grads, save_best, grad_mode, optim, scheduler)\u001b[0m\n\u001b[0;32m    307\u001b[0m     vfe \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mvfe(state)\n\u001b[0;32m    308\u001b[0m     \u001b[38;5;66;03m# Plots computation graph for vfe, for debugging\u001b[39;00m\n\u001b[0;32m    309\u001b[0m     \u001b[38;5;66;03m# if epoch == 0 and batch_i == 0:\u001b[39;00m\n\u001b[0;32m    310\u001b[0m     \u001b[38;5;66;03m#     make_dot(vfe).render(\"vfe\", format=\"png\")\u001b[39;00m\n\u001b[1;32m--> 311\u001b[0m     \u001b[43mvfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    313\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(model\u001b[38;5;241m.\u001b[39mlayers):\n\u001b[0;32m    314\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(layer, FCLI):\n\u001b[0;32m    315\u001b[0m         \u001b[38;5;66;03m# Hebbian update to reduce correlations between neurons\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\joeag\\Documents\\venvs\\ml-env\\Lib\\site-packages\\torch\\_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    477\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    478\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    479\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    480\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    485\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    486\u001b[0m     )\n\u001b[1;32m--> 487\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    488\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    489\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\joeag\\Documents\\venvs\\ml-env\\Lib\\site-packages\\torch\\autograd\\__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    195\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    197\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[0;32m    198\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    199\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 200\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    201\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    202\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Train Loop\n",
    "BATCH_SIZE = 1000\n",
    "\n",
    "log_dir = f'examples/fashion_mnist/logs/conv/{model_name}'\n",
    "\n",
    "NUM_EPOCHS = 50\n",
    "train(\n",
    "    model, \n",
    "    train_dataset, \n",
    "    val_dataset, \n",
    "    NUM_EPOCHS, \n",
    "    lr=0.001,\n",
    "    c_lr=0.01,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    reg_coeff=0.02,\n",
    "    flatten=False,\n",
    "    save_best=False,\n",
    "    optim='AdamW',\n",
    "    grad_mode='auto',\n",
    "    log_dir=log_dir,\n",
    "    minimal_stats=True,\n",
    "    scheduler='ReduceLROnPlateau',\n",
    ")\n",
    "\n",
    "# NUM_EPOCHS = 20\n",
    "# train(\n",
    "#     model, \n",
    "#     train_dataset, \n",
    "#     val_dataset, \n",
    "#     NUM_EPOCHS, \n",
    "#     lr=0.0,\n",
    "#     c_lr=0.01,\n",
    "#     batch_size=BATCH_SIZE,\n",
    "#     reg_coeff=0.02,\n",
    "#     flatten=False,\n",
    "#     save_best=False,\n",
    "#     optim='AdamW',\n",
    "#     grad_mode='auto',\n",
    "#     log_dir=log_dir,\n",
    "#     minimal_stats=True,\n",
    "#     scheduler='ReduceLROnPlateau',\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel = 3\n",
    "padding = 1\n",
    "stride = 1\n",
    "in_features = 1\n",
    "out_features = 4\n",
    "\n",
    "conv = torch.nn.Conv2d(in_features, out_features, kernel, stride, padding)\n",
    "convt = torch.nn.ConvTranspose2d(out_features, in_features, kernel, stride, padding)\n",
    "\n",
    "x = torch.randn(1, 1, 28, 28)\n",
    "y = conv(x)\n",
    "x_hat = convt(y)\n",
    "\n",
    "print(f\"x.shape: {x.shape}\")\n",
    "print(f\"y.shape: {y.shape}\")\n",
    "print(f\"x_hat.shape: {x_hat.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets = torch.arange(NUM_CLASSES).to(device)\n",
    "y = format_y(targets, NUM_CLASSES)\n",
    "_, state = model(y=y, lmda=0.00)\n",
    "images = state[-1]['x']\n",
    "\n",
    "fig, axes = plt.subplots(1, 10, figsize=(15, 5))\n",
    "for i, ax in enumerate(axes):\n",
    "    img = images[i]\n",
    "    ax.imshow(img.detach().squeeze().cpu().view(28,28), cmap='gray')\n",
    "    ax.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(stats['val_acc'])\n",
    "plt.title(\"Validation Accuracy\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run():\n",
    "    x = torch.randn(1000, 1, 28, 28).to(device)\n",
    "    out = model(x, steps=100)\n",
    "\n",
    "%timeit run()\n",
    "for p in model.parameters():\n",
    "    p.requires_grad = False\n",
    "%timeit run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Current Accuracy and Error Rate\n",
    "acc = accuracy(model, val_dataset, steps=0)\n",
    "error = 100 * (1 - acc)\n",
    "print(f'Current Val Acc: {acc} | error_rate: {error:0.2f} %')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Shows statistics over multiple models. models = [model1, model2, ...]\n",
    "\n",
    "\n",
    "# train_vfes = torch.tensor([stats[i]['train_vfe'][-1] for i in range(num_models)])\n",
    "# val_vfes = torch.tensor([stats[i]['val_vfe'][-1] for i in range(num_models)])\n",
    "# val_accs = torch.tensor([stats[i]['val_acc'][-1] for i in range(num_models)])\n",
    "\n",
    "# # Show statistics across models, std is nan if num_models = 1\n",
    "# print(f\"Tra VFE - mean: {train_vfes.mean():.3f} | std: {train_vfes.std():.3f} | min: {train_vfes.min():.3f} | max: {train_vfes.max():.3f}\")\n",
    "# print(f\"Val VFE - mean: {val_vfes.mean():.3f} | std: {val_vfes.std():.3f} | min: {val_vfes.min():.3f} | max: {val_vfes.max():.3f}\")\n",
    "# print(f\"Val Acc - mean: {val_accs.mean():.3f} | std: {val_accs.std():.3f} | min: {val_accs.min():.3f} | max: {val_accs.max():.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pclib.utils.functional import format_y\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=512, shuffle=True)\n",
    "images, y = next(iter(train_loader))\n",
    "x = images\n",
    "y = format_y(y, 10)\n",
    "\n",
    "track_vfe(model, x, y, 100, plot_Es=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert isinstance(model.layers[0], PrecisionWeighted), \"This cell only works for PrecisionWeighted layers\"\n",
    "# show diag of weight_var matrix as 28x28 image for layer 0\n",
    "model = model\n",
    "layer = 0\n",
    "weight_var = model.layers[0].weight_var.detach().cpu().numpy()\n",
    "# weight_var = model.layers[-1].weight_var.diag().reshape(28,28).detach().cpu().numpy()\n",
    "plt.imshow(weight_var, cmap='gray')\n",
    "plt.colorbar()\n",
    "plt.show()\n",
    "weight_var.min(), weight_var.max()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
