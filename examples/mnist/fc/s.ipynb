{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from pclib.nn.models import FCClassifier\n",
    "from pclib.nn.layers import FCPW\n",
    "from pclib.optim.train import train\n",
    "from pclib.optim.eval import track_vfe, accuracy\n",
    "from pclib.utils.functional import format_y, reTanh\n",
    "from pclib.utils.customdataset import PreloadedDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seed = 42\n",
    "os.environ['CUBLAS_WORKSPACE_CONFIG'] = ':4096:8'\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                        \r"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(seed)\n",
    "class TanhTransform(object):\n",
    "    def __init__(self, a=1., b=0., c=1.0):\n",
    "        self.a = a\n",
    "        self.b = b\n",
    "        self.c = c\n",
    "\n",
    "    def __call__(self, img):\n",
    "        return ((img * self.a).tanh() + self.b) * self.c\n",
    "\n",
    "class InvTanhTransform(object):\n",
    "    def __call__(self, img):\n",
    "        num = 1 + img\n",
    "        div = (1 - img).clamp(min=1e-6)\n",
    "        m = 0.5 * torch.log(num / div)\n",
    "        return m\n",
    "\n",
    "class SigmoidTransform(object):\n",
    "    def __call__(self, img):\n",
    "        return img.sigmoid()\n",
    "    \n",
    "class ReLUTanhTransform(object):\n",
    "    def __call__(self, img):\n",
    "        return F.relu(img.tanh())\n",
    "\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.RandomAffine(degrees=10, translate=(0.1, 0.1), scale=(0.9, 1.1)),                                \n",
    "    transforms.Normalize((0.1307,), (0.3081,)),\n",
    "    # TanhTransform(a=0.5),\n",
    "    TanhTransform(a=1.0, b=1.0, c=0.5),\n",
    "])\n",
    "\n",
    "val_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    # InvTanhTransform(),\n",
    "    transforms.Normalize((0.1307,), (0.3081,)),\n",
    "    # SigmoidTransform(),\n",
    "    # TanhTransform(a=0.5),\n",
    "    TanhTransform(a=1.0, b=1.0, c=0.5),\n",
    "])\n",
    "\n",
    "dataset = datasets.MNIST('../Datasets/', train=True, download=False, transform=transforms.ToTensor())\n",
    "# # shorten dataset\n",
    "# length = 1000\n",
    "# dataset = torch.utils.data.Subset(dataset, range(length))\n",
    "\n",
    "VAL_RATIO = 0.2\n",
    "val_len = int(len(dataset) * VAL_RATIO)\n",
    "train_len = len(dataset) - val_len\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_len, val_len])\n",
    "train_dataset = PreloadedDataset.from_dataset(train_dataset, train_transform, device)\n",
    "val_dataset = PreloadedDataset.from_dataset(val_dataset, val_transform, device)\n",
    "INPUT_SHAPE = 784\n",
    "NUM_CLASSES = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1500x500 with 10 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max value:  tensor(0.9965, device='cuda:0')\n",
      "Min value:  tensor(0.2998, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "# Show some images\n",
    "fig, axes = plt.subplots(1, 10, figsize=(15, 5))\n",
    "for i, ax in enumerate(axes):\n",
    "    img, label = train_dataset[i]\n",
    "    ax.imshow(img.squeeze().cpu(), cmap='gray')\n",
    "    ax.axis('off')\n",
    "plt.show()\n",
    "\n",
    "# print max min values\n",
    "print('Max value: ', train_dataset.transformed_images.max())\n",
    "print('Min value: ', train_dataset.transformed_images.min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "model_name = 'fc_classifier'\n",
    "model = FCClassifier(\n",
    "    in_features=INPUT_SHAPE, \n",
    "    num_classes=NUM_CLASSES, \n",
    "    hidden_sizes=[200, 200],\n",
    "    bias=True, \n",
    "    symmetric=True, \n",
    "    precision_weighted=False,\n",
    "    actv_fn=F.tanh,\n",
    "    steps=100,\n",
    "    gamma=0.1,\n",
    "    ).to(device)\n",
    "stats = None\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                           \r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 7\u001b[0m\n\u001b[0;32m      4\u001b[0m BATCH_SIZE \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1000\u001b[39m\n\u001b[0;32m      5\u001b[0m REG_COEFF \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1.0\u001b[39m\n\u001b[1;32m----> 7\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mval_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43mNUM_EPOCHS\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43mLEARNING_RATE\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mBATCH_SIZE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreg_coeff\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mREG_COEFF\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     16\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstats\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstats\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     17\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mAdamW\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     18\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmanual\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     19\u001b[0m \u001b[43m    \u001b[49m\u001b[43msave_best\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     20\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# assert_grads=True,\u001b[39;49;00m\n\u001b[0;32m     21\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m     22\u001b[0m LEARNING_RATE \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0001\u001b[39m\n\u001b[0;32m     23\u001b[0m NUM_EPOCHS \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\joeag\\Documents\\pclib\\pclib\\optim\\train.py:335\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(model, model_name, train_data, val_data, num_epochs, lr, c_lr, batch_size, reg_coeff, flatten, neg_coeff, untr_coeff, stats, minimal_stats, track_corr, assert_grads, val_grads, save_best, grad_mode, optim)\u001b[0m\n\u001b[0;32m    333\u001b[0m \u001b[38;5;66;03m# Collects statistics for validation data if it exists\u001b[39;00m\n\u001b[0;32m    334\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m val_data \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 335\u001b[0m     val_vfe, val_acc \u001b[38;5;241m=\u001b[39m \u001b[43mval_pass\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mflatten\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_grads\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    336\u001b[0m     stats[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_vfe\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m val_vfe\u001b[38;5;241m.\u001b[39mitem()\n\u001b[0;32m    337\u001b[0m     stats[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_acc\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m val_acc\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[1;32mc:\\Users\\joeag\\Documents\\pclib\\pclib\\optim\\train.py:141\u001b[0m, in \u001b[0;36mval_pass\u001b[1;34m(model, val_loader, flatten, allow_grads)\u001b[0m\n\u001b[0;32m    138\u001b[0m     x \u001b[38;5;241m=\u001b[39m images\n\u001b[0;32m    140\u001b[0m \u001b[38;5;66;03m# Forward pass\u001b[39;00m\n\u001b[1;32m--> 141\u001b[0m out, val_state \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    142\u001b[0m val_vfe \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mvfe(val_state, batch_reduction\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msum\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    143\u001b[0m val_correct \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (out\u001b[38;5;241m.\u001b[39margmax(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m==\u001b[39m target)\u001b[38;5;241m.\u001b[39msum()\n",
      "File \u001b[1;32mc:\\Users\\joeag\\Documents\\venvs\\ml-env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\joeag\\Documents\\pclib\\pclib\\nn\\models\\fc_classifier.py:271\u001b[0m, in \u001b[0;36mFCClassifier.forward\u001b[1;34m(self, obs, y, steps)\u001b[0m\n\u001b[0;32m    269\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(steps):\n\u001b[0;32m    270\u001b[0m     temp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcalc_temp(i, steps)\n\u001b[1;32m--> 271\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemp\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    273\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_output(state)\n\u001b[0;32m    275\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m out, state\n",
      "File \u001b[1;32mc:\\Users\\joeag\\Documents\\pclib\\pclib\\nn\\models\\fc_classifier.py:165\u001b[0m, in \u001b[0;36mFCClassifier.step\u001b[1;34m(self, state, obs, y, temp)\u001b[0m\n\u001b[0;32m    163\u001b[0m             layer\u001b[38;5;241m.\u001b[39mupdate_x(state[i], e_below, temp\u001b[38;5;241m=\u001b[39mtemp)\n\u001b[0;32m    164\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m--> 165\u001b[0m     pred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayers\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    166\u001b[0m     layer\u001b[38;5;241m.\u001b[39mupdate_e(state[i], pred, temp\u001b[38;5;241m=\u001b[39mtemp)\n",
      "File \u001b[1;32mc:\\Users\\joeag\\Documents\\pclib\\pclib\\nn\\layers\\fc.py:140\u001b[0m, in \u001b[0;36mFC.predict\u001b[1;34m(self, state)\u001b[0m\n\u001b[0;32m    129\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpredict\u001b[39m(\u001b[38;5;28mself\u001b[39m, state):\n\u001b[0;32m    130\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    131\u001b[0m \u001b[38;5;124;03m    | Calculates a prediction of state['x'] in the layer below.\u001b[39;00m\n\u001b[0;32m    132\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    137\u001b[0m \u001b[38;5;124;03m        | pred (torch.Tensor): Prediction of state['x'] in the layer below.\u001b[39;00m\n\u001b[0;32m    138\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 140\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mactv_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mx\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdetach\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight_td\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Train Loop\n",
    "LEARNING_RATE = 0.001\n",
    "NUM_EPOCHS = 10\n",
    "BATCH_SIZE = 1000\n",
    "REG_COEFF = 1.0\n",
    "\n",
    "train(\n",
    "    model, \n",
    "    model_name,\n",
    "    train_dataset, \n",
    "    val_dataset, \n",
    "    NUM_EPOCHS, \n",
    "    LEARNING_RATE, \n",
    "    batch_size=BATCH_SIZE,\n",
    "    reg_coeff=REG_COEFF,\n",
    "    stats=stats,\n",
    "    optim='AdamW',\n",
    "    grad_mode='manual',\n",
    "    save_best=False,\n",
    "    # assert_grads=True,\n",
    ")\n",
    "LEARNING_RATE = 0.0001\n",
    "NUM_EPOCHS = 0\n",
    "train(\n",
    "    model, \n",
    "    model_name,\n",
    "    train_dataset, \n",
    "    val_dataset, \n",
    "    NUM_EPOCHS, \n",
    "    LEARNING_RATE, \n",
    "    batch_size=BATCH_SIZE,\n",
    "    reg_coeff=REG_COEFF,\n",
    "    stats=stats,\n",
    "    optim='AdamW',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets = torch.arange(NUM_CLASSES).to(device)\n",
    "images = model.generate(targets)\n",
    "\n",
    "fig, axes = plt.subplots(1, 10, figsize=(15, 5))\n",
    "for i, ax in enumerate(axes):\n",
    "    img = images[i]\n",
    "    ax.imshow(img.detach().squeeze().cpu().view(28,28), cmap='gray')\n",
    "    ax.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(stats['val_acc'])\n",
    "plt.title(\"Validation Accuracy\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Current Accuracy and Error Rate\n",
    "acc = accuracy(model, val_dataset, steps=0)\n",
    "error = 100 * (1 - acc)\n",
    "print(f'Current Val Acc: {acc} | error_rate: {error:0.2f} %')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Shows statistics over multiple models. models = [model1, model2, ...]\n",
    "\n",
    "\n",
    "# train_vfes = torch.tensor([stats[i]['train_vfe'][-1] for i in range(num_models)])\n",
    "# val_vfes = torch.tensor([stats[i]['val_vfe'][-1] for i in range(num_models)])\n",
    "# val_accs = torch.tensor([stats[i]['val_acc'][-1] for i in range(num_models)])\n",
    "\n",
    "# # Show statistics across models, std is nan if num_models = 1\n",
    "# print(f\"Tra VFE - mean: {train_vfes.mean():.3f} | std: {train_vfes.std():.3f} | min: {train_vfes.min():.3f} | max: {train_vfes.max():.3f}\")\n",
    "# print(f\"Val VFE - mean: {val_vfes.mean():.3f} | std: {val_vfes.std():.3f} | min: {val_vfes.min():.3f} | max: {val_vfes.max():.3f}\")\n",
    "# print(f\"Val Acc - mean: {val_accs.mean():.3f} | std: {val_accs.std():.3f} | min: {val_accs.min():.3f} | max: {val_accs.max():.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pclib.utils.functional import format_y\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=512, shuffle=True)\n",
    "images, y = next(iter(train_loader))\n",
    "x = images.view(images.shape[0], -1)\n",
    "y = format_y(y, 10)\n",
    "vfes, X, E = track_vfe(model, x, steps=100, plot_Es=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(E[0][0], E[0][-1]), (X[0][0], X[0][1], X[0][2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert isinstance(model.layers[0], PrecisionWeighted), \"This cell only works for PrecisionWeighted layers\"\n",
    "# show diag of weight_var matrix as 28x28 image for layer 0\n",
    "model = model\n",
    "layer = 0\n",
    "weight_var = model.layers[0].weight_var.detach().cpu().numpy()\n",
    "# weight_var = model.layers[-1].weight_var.diag().reshape(28,28).detach().cpu().numpy()\n",
    "plt.imshow(weight_var, cmap='gray')\n",
    "plt.colorbar()\n",
    "plt.show()\n",
    "weight_var.min(), weight_var.max()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
