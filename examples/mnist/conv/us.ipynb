{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from pclib.nn.models import ConvClassifierUs\n",
    "from pclib.nn.layers import Conv2d\n",
    "from pclib.optim.train import train\n",
    "from pclib.optim.eval import track_vfe, accuracy\n",
    "from pclib.utils.functional import format_y, identity\n",
    "from pclib.utils.customdataset import PreloadedDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                        \r"
     ]
    }
   ],
   "source": [
    "\n",
    "class TanhTransform(object):\n",
    "    def __init__(self, a=1., b=0., c=1.0):\n",
    "        self.a = a\n",
    "        self.b = b\n",
    "        self.c = c\n",
    "\n",
    "    def __call__(self, img):\n",
    "        return ((img * self.a).tanh() + self.b) * self.c\n",
    "\n",
    "class PadTransform(object):\n",
    "    def __call__(self, img):\n",
    "        return F.pad(img, (2, 2, 2, 2), mode='constant', value=0)\n",
    "\n",
    "class InvTanhTransform(object):\n",
    "    def __call__(self, img):\n",
    "        num = 1 + img\n",
    "        div = (1 - img).clamp(min=1e-6)\n",
    "        m = 0.5 * torch.log(num / div)\n",
    "        return m\n",
    "\n",
    "class SigmoidTransform(object):\n",
    "    def __call__(self, img):\n",
    "        return img.sigmoid()\n",
    "    \n",
    "class ReLUTanhTransform(object):\n",
    "    def __call__(self, img):\n",
    "        return F.relu(img.tanh())\n",
    "\n",
    "class AddTransform(object):\n",
    "    def __init__(self, add):\n",
    "        self.add = add\n",
    "\n",
    "    def __call__(self, img):\n",
    "        return img + self.add\n",
    "\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    # PadTransform(),\n",
    "    transforms.RandomAffine(degrees=10, translate=(0.1, 0.1), scale=(0.9, 1.1)),                                \n",
    "    transforms.Normalize((0.1307,), (0.3081,)),\n",
    "    # AddTransform(0.4242),\n",
    "])\n",
    "\n",
    "val_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    # PadTransform(),\n",
    "    # InvTanhTransform(),\n",
    "    transforms.Normalize((0.1307,), (0.3081,)),\n",
    "    # AddTransform(0.4242),\n",
    "    # SigmoidTransform(),\n",
    "    # TanhTransform(a=2.0),\n",
    "    # TanhTransform(a=1.0, b=1.0, c=0.5),\n",
    "])\n",
    "\n",
    "dataset = datasets.MNIST('../Datasets/', train=True, download=False, transform=transforms.ToTensor())\n",
    "# # shorten dataset\n",
    "# length = 1000\n",
    "# dataset = torch.utils.data.Subset(dataset, range(length))\n",
    "\n",
    "VAL_RATIO = 0.2\n",
    "val_len = int(len(dataset) * VAL_RATIO)\n",
    "train_len = len(dataset) - val_len\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_len, val_len])\n",
    "train_dataset = PreloadedDataset.from_dataset(train_dataset, train_transform, device)\n",
    "val_dataset = PreloadedDataset.from_dataset(val_dataset, val_transform, device)\n",
    "INPUT_SHAPE = 784\n",
    "NUM_CLASSES = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1500x500 with 10 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max value:  tensor(2.8215, device='cuda:0')\n",
      "Min value:  tensor(-0.4242, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "# Show some images\n",
    "fig, axes = plt.subplots(1, 10, figsize=(15, 5))\n",
    "for i, ax in enumerate(axes):\n",
    "    img, label = train_dataset[i]\n",
    "    ax.imshow(img.squeeze().cpu(), cmap='gray')\n",
    "    ax.axis('off')\n",
    "plt.show()\n",
    "\n",
    "# print max min values\n",
    "print('Max value: ', train_dataset.transformed_images.max())\n",
    "print('Min value: ', train_dataset.transformed_images.min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "model_name = 'nostride-nopool-128'\n",
    "model = ConvClassifierUs(\n",
    "    bias=True,\n",
    "    symmetric=True, \n",
    "    precision_weighted=False,\n",
    "    actv_fn=F.tanh,\n",
    "    steps=100,\n",
    "    gamma=0.1,\n",
    "    ).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                      \r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\joeag\\Documents\\pclib\\pclib\\optim\\train.py:301\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(model, train_data, val_data, num_epochs, lr, c_lr, batch_size, reg_coeff, flatten, neg_coeff, untr_coeff, log_dir, minimal_stats, track_corr, assert_grads, val_grads, save_best, grad_mode, optim, scheduler)\u001b[0m\n\u001b[0;32m    300\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 301\u001b[0m     out, state \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    302\u001b[0m \u001b[38;5;66;03m# catch typeerror if model is not supervised\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\joeag\\Documents\\venvs\\ml-env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n",
      "\u001b[1;31mTypeError\u001b[0m: ConvClassifierUs.forward() got an unexpected keyword argument 'y'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 7\u001b[0m\n\u001b[0;32m      4\u001b[0m log_dir \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mexamples/mnist/logs/conv_arch/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m      6\u001b[0m NUM_EPOCHS \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m30\u001b[39m\n\u001b[1;32m----> 7\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mval_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mNUM_EPOCHS\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.001\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43mc_lr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.01\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mBATCH_SIZE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreg_coeff\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.02\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     16\u001b[0m \u001b[43m    \u001b[49m\u001b[43mflatten\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     17\u001b[0m \u001b[43m    \u001b[49m\u001b[43msave_best\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     18\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mAdamW\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     19\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mauto\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     20\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlog_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlog_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     21\u001b[0m \u001b[43m    \u001b[49m\u001b[43mminimal_stats\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     22\u001b[0m \u001b[43m    \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mReduceLROnPlateau\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     23\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m     25\u001b[0m NUM_EPOCHS \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m20\u001b[39m\n\u001b[0;32m     26\u001b[0m train(\n\u001b[0;32m     27\u001b[0m     model, \n\u001b[0;32m     28\u001b[0m     train_dataset, \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     41\u001b[0m     scheduler\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mReduceLROnPlateau\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m     42\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\joeag\\Documents\\pclib\\pclib\\optim\\train.py:304\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(model, train_data, val_data, num_epochs, lr, c_lr, batch_size, reg_coeff, flatten, neg_coeff, untr_coeff, log_dir, minimal_stats, track_corr, assert_grads, val_grads, save_best, grad_mode, optim, scheduler)\u001b[0m\n\u001b[0;32m    302\u001b[0m \u001b[38;5;66;03m# catch typeerror if model is not supervised\u001b[39;00m\n\u001b[0;32m    303\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m--> 304\u001b[0m     out, state \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    306\u001b[0m model\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m    307\u001b[0m vfe \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mvfe(state)\n",
      "File \u001b[1;32mc:\\Users\\joeag\\Documents\\venvs\\ml-env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\joeag\\Documents\\pclib\\pclib\\nn\\models\\conv_classifier_us.py:121\u001b[0m, in \u001b[0;36mConvClassifierUs.forward\u001b[1;34m(self, obs, steps)\u001b[0m\n\u001b[0;32m    119\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(steps):\n\u001b[0;32m    120\u001b[0m     temp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcalc_temp(i, steps)\n\u001b[1;32m--> 121\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemp\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    123\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_output(state)\n\u001b[0;32m    125\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m out, state\n",
      "File \u001b[1;32mc:\\Users\\joeag\\Documents\\pclib\\pclib\\nn\\models\\conv_classifier.py:122\u001b[0m, in \u001b[0;36mConvClassifier.step\u001b[1;34m(self, state, obs, y, temp)\u001b[0m\n\u001b[0;32m    120\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m obs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m: \u001b[38;5;66;03m# Don't update bottom x if obs is given\u001b[39;00m\n\u001b[0;32m    121\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m y \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m: \u001b[38;5;66;03m# Don't update top x if y is given\u001b[39;00m\n\u001b[1;32m--> 122\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mwith\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mno_grad\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m    123\u001b[0m \u001b[43m            \u001b[49m\u001b[43mlayer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate_x\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43me_below\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43md_pred\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43md_pred\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtemp\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    124\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\joeag\\Documents\\venvs\\ml-env\\Lib\\site-packages\\torch\\autograd\\grad_mode.py:57\u001b[0m, in \u001b[0;36mno_grad.__exit__\u001b[1;34m(self, exc_type, exc_value, traceback)\u001b[0m\n\u001b[0;32m     54\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprev \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mis_grad_enabled()\n\u001b[0;32m     55\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m---> 57\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__exit__\u001b[39m(\u001b[38;5;28mself\u001b[39m, exc_type: Any, exc_value: Any, traceback: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     58\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprev)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Train Loop\n",
    "BATCH_SIZE = 1000\n",
    "\n",
    "log_dir = f'examples/mnist/logs/conv_arch/{model_name}'\n",
    "\n",
    "NUM_EPOCHS = 30\n",
    "train(\n",
    "    model, \n",
    "    train_dataset, \n",
    "    val_dataset, \n",
    "    NUM_EPOCHS, \n",
    "    lr=0.001,\n",
    "    c_lr=0.01,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    reg_coeff=0.02,\n",
    "    flatten=False,\n",
    "    save_best=False,\n",
    "    optim='AdamW',\n",
    "    grad_mode='auto',\n",
    "    log_dir=log_dir,\n",
    "    minimal_stats=True,\n",
    "    scheduler='ReduceLROnPlateau',\n",
    ")\n",
    "\n",
    "NUM_EPOCHS = 20\n",
    "train(\n",
    "    model, \n",
    "    train_dataset, \n",
    "    val_dataset, \n",
    "    NUM_EPOCHS, \n",
    "    lr=0.0,\n",
    "    c_lr=0.01,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    reg_coeff=0.02,\n",
    "    flatten=False,\n",
    "    save_best=False,\n",
    "    optim='AdamW',\n",
    "    grad_mode='auto',\n",
    "    log_dir=log_dir,\n",
    "    minimal_stats=True,\n",
    "    scheduler='ReduceLROnPlateau',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel = 3\n",
    "padding = 1\n",
    "stride = 1\n",
    "in_features = 1\n",
    "out_features = 4\n",
    "\n",
    "conv = torch.nn.Conv2d(in_features, out_features, kernel, stride, padding)\n",
    "convt = torch.nn.ConvTranspose2d(out_features, in_features, kernel, stride, padding)\n",
    "\n",
    "x = torch.randn(1, 1, 28, 28)\n",
    "y = conv(x)\n",
    "x_hat = convt(y)\n",
    "\n",
    "print(f\"x.shape: {x.shape}\")\n",
    "print(f\"y.shape: {y.shape}\")\n",
    "print(f\"x_hat.shape: {x_hat.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets = torch.arange(NUM_CLASSES).to(device)\n",
    "y = format_y(targets, NUM_CLASSES)\n",
    "_, state = model(y=y, lmda=0.00)\n",
    "images = state[-1]['x']\n",
    "\n",
    "fig, axes = plt.subplots(1, 10, figsize=(15, 5))\n",
    "for i, ax in enumerate(axes):\n",
    "    img = images[i]\n",
    "    ax.imshow(img.detach().squeeze().cpu().view(28,28), cmap='gray')\n",
    "    ax.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(stats['val_acc'])\n",
    "plt.title(\"Validation Accuracy\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run():\n",
    "    x = torch.randn(1000, 1, 28, 28).to(device)\n",
    "    out = model(x, steps=100)\n",
    "\n",
    "%timeit run()\n",
    "for p in model.parameters():\n",
    "    p.requires_grad = False\n",
    "%timeit run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Current Accuracy and Error Rate\n",
    "acc = accuracy(model, val_dataset, steps=0)\n",
    "error = 100 * (1 - acc)\n",
    "print(f'Current Val Acc: {acc} | error_rate: {error:0.2f} %')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Shows statistics over multiple models. models = [model1, model2, ...]\n",
    "\n",
    "\n",
    "# train_vfes = torch.tensor([stats[i]['train_vfe'][-1] for i in range(num_models)])\n",
    "# val_vfes = torch.tensor([stats[i]['val_vfe'][-1] for i in range(num_models)])\n",
    "# val_accs = torch.tensor([stats[i]['val_acc'][-1] for i in range(num_models)])\n",
    "\n",
    "# # Show statistics across models, std is nan if num_models = 1\n",
    "# print(f\"Tra VFE - mean: {train_vfes.mean():.3f} | std: {train_vfes.std():.3f} | min: {train_vfes.min():.3f} | max: {train_vfes.max():.3f}\")\n",
    "# print(f\"Val VFE - mean: {val_vfes.mean():.3f} | std: {val_vfes.std():.3f} | min: {val_vfes.min():.3f} | max: {val_vfes.max():.3f}\")\n",
    "# print(f\"Val Acc - mean: {val_accs.mean():.3f} | std: {val_accs.std():.3f} | min: {val_accs.min():.3f} | max: {val_accs.max():.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pclib.utils.functional import format_y\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=512, shuffle=True)\n",
    "images, y = next(iter(train_loader))\n",
    "x = images\n",
    "y = format_y(y, 10)\n",
    "\n",
    "track_vfe(model, x, y, 100, plot_Es=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert isinstance(model.layers[0], PrecisionWeighted), \"This cell only works for PrecisionWeighted layers\"\n",
    "# show diag of weight_var matrix as 28x28 image for layer 0\n",
    "model = model\n",
    "layer = 0\n",
    "weight_var = model.layers[0].weight_var.detach().cpu().numpy()\n",
    "# weight_var = model.layers[-1].weight_var.diag().reshape(28,28).detach().cpu().numpy()\n",
    "plt.imshow(weight_var, cmap='gray')\n",
    "plt.colorbar()\n",
    "plt.show()\n",
    "weight_var.min(), weight_var.max()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
