{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "from pclib.nn.models import ConvClassifierUs\n",
    "from pclib.nn.layers import Conv2d\n",
    "from pclib.optim.train import train\n",
    "from pclib.optim.eval import track_vfe, accuracy\n",
    "from pclib.utils.functional import format_y, identity\n",
    "from pclib.utils.customdataset import PreloadedDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seed = 42\n",
    "os.environ['CUBLAS_WORKSPACE_CONFIG'] = ':4096:8'\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                        \r"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(seed)\n",
    "class TanhTransform(object):\n",
    "    def __init__(self, a=1., b=0., c=1.0):\n",
    "        self.a = a\n",
    "        self.b = b\n",
    "        self.c = c\n",
    "\n",
    "    def __call__(self, img):\n",
    "        return ((img * self.a).tanh() + self.b) * self.c\n",
    "\n",
    "class PadTransform(object):\n",
    "    def __call__(self, img):\n",
    "        return F.pad(img, (2, 2, 2, 2), mode='constant', value=0)\n",
    "\n",
    "class InvTanhTransform(object):\n",
    "    def __call__(self, img):\n",
    "        num = 1 + img\n",
    "        div = (1 - img).clamp(min=1e-6)\n",
    "        m = 0.5 * torch.log(num / div)\n",
    "        return m\n",
    "\n",
    "class SigmoidTransform(object):\n",
    "    def __call__(self, img):\n",
    "        return img.sigmoid()\n",
    "    \n",
    "class ReLUTanhTransform(object):\n",
    "    def __call__(self, img):\n",
    "        return F.relu(img.tanh())\n",
    "\n",
    "class AddTransform(object):\n",
    "    def __init__(self, add):\n",
    "        self.add = add\n",
    "\n",
    "    def __call__(self, img):\n",
    "        return img + self.add\n",
    "\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    PadTransform(),\n",
    "    # transforms.RandomAffine(degrees=10, translate=(0.1, 0.1), scale=(0.9, 1.1)),                                \n",
    "    transforms.Normalize((0.1307,), (0.3081,)),\n",
    "    # AddTransform(0.4242),\n",
    "])\n",
    "\n",
    "val_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    PadTransform(),\n",
    "    # InvTanhTransform(),\n",
    "    transforms.Normalize((0.1307,), (0.3081,)),\n",
    "    # AddTransform(0.4242),\n",
    "    # SigmoidTransform(),\n",
    "    # TanhTransform(a=2.0),\n",
    "    # TanhTransform(a=1.0, b=1.0, c=0.5),\n",
    "])\n",
    "\n",
    "dataset = datasets.MNIST('../Datasets/', train=True, download=False, transform=transforms.ToTensor())\n",
    "# # shorten dataset\n",
    "# length = 1000\n",
    "# dataset = torch.utils.data.Subset(dataset, range(length))\n",
    "\n",
    "VAL_RATIO = 0.2\n",
    "val_len = int(len(dataset) * VAL_RATIO)\n",
    "train_len = len(dataset) - val_len\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_len, val_len])\n",
    "train_dataset = PreloadedDataset.from_dataset(train_dataset, train_transform, device)\n",
    "val_dataset = PreloadedDataset.from_dataset(val_dataset, val_transform, device)\n",
    "INPUT_SHAPE = 784\n",
    "NUM_CLASSES = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1500x500 with 10 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max value:  tensor(2.8215, device='cuda:0')\n",
      "Min value:  tensor(-0.4242, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "# Show some images\n",
    "fig, axes = plt.subplots(1, 10, figsize=(15, 5))\n",
    "for i, ax in enumerate(axes):\n",
    "    img, label = train_dataset[i]\n",
    "    ax.imshow(img.squeeze().cpu(), cmap='gray')\n",
    "    ax.axis('off')\n",
    "plt.show()\n",
    "\n",
    "# print max min values\n",
    "print('Max value: ', train_dataset.transformed_images.max())\n",
    "print('Min value: ', train_dataset.transformed_images.min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(seed)\n",
    "\n",
    "model_name = 'identity-2phase-norm_weights-no_mom'\n",
    "model = ConvClassifierUs(\n",
    "    bias=True,\n",
    "    symmetric=True, \n",
    "    actv_fn=identity,\n",
    "    steps=60,\n",
    "    gamma=0.1,\n",
    "    ).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                                                      \r"
     ]
    }
   ],
   "source": [
    "# Train Loop\n",
    "BATCH_SIZE = 1000\n",
    "\n",
    "log_dir = f'examples/mnist/logs/conv_grad/{model_name}'\n",
    "\n",
    "NUM_EPOCHS = 50\n",
    "train(\n",
    "    model, \n",
    "    train_dataset, \n",
    "    val_dataset, \n",
    "    NUM_EPOCHS, \n",
    "    lr=0.00001,\n",
    "    c_lr=0.0,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    reg_coeff=0.02,\n",
    "    flatten=False,\n",
    "    save_best=False,\n",
    "    optim='AdamW',\n",
    "    grad_mode='auto',\n",
    "    log_dir=log_dir,\n",
    "    minimal_stats=True,\n",
    "    scheduler='ReduceLROnPlateau',\n",
    "    track_corr=True,\n",
    "    track_sparsity=True,\n",
    "    no_momentum=True,\n",
    ")\n",
    "train(\n",
    "    model, \n",
    "    train_dataset, \n",
    "    val_dataset, \n",
    "    NUM_EPOCHS, \n",
    "    lr=0.0,\n",
    "    c_lr=0.001,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    reg_coeff=0.02,\n",
    "    flatten=False,\n",
    "    save_best=False,\n",
    "    optim='AdamW',\n",
    "    grad_mode='auto',\n",
    "    log_dir=log_dir,\n",
    "    minimal_stats=True,\n",
    "    scheduler='ReduceLROnPlateau',\n",
    "    track_corr=True,\n",
    "    track_sparsity=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([10, 2]), torch.Size([10, 2, 1, 1]))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "in_features = 2\n",
    "out_features = 10\n",
    "conv = torch.nn.Conv2d(in_features, out_features, kernel_size=1, bias=True).to(device)\n",
    "linear = torch.nn.Linear(in_features, out_features, bias=True).to(device)\n",
    "\n",
    "linear.weight.data.shape, conv.weight.data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel = 3\n",
    "padding = 1\n",
    "stride = 1\n",
    "in_features = 1\n",
    "out_features = 4\n",
    "\n",
    "conv = torch.nn.Conv2d(in_features, out_features, kernel, stride, padding)\n",
    "convt = torch.nn.ConvTranspose2d(out_features, in_features, kernel, stride, padding)\n",
    "\n",
    "x = torch.randn(1, 1, 28, 28)\n",
    "y = conv(x)\n",
    "x_hat = convt(y)\n",
    "\n",
    "print(f\"x.shape: {x.shape}\")\n",
    "print(f\"y.shape: {y.shape}\")\n",
    "print(f\"x_hat.shape: {x_hat.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets = torch.arange(NUM_CLASSES).to(device)\n",
    "y = format_y(targets, NUM_CLASSES)\n",
    "_, state = model(y=y, lmda=0.00)\n",
    "images = state[-1]['x']\n",
    "\n",
    "fig, axes = plt.subplots(1, 10, figsize=(15, 5))\n",
    "for i, ax in enumerate(axes):\n",
    "    img = images[i]\n",
    "    ax.imshow(img.detach().squeeze().cpu().view(28,28), cmap='gray')\n",
    "    ax.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(stats['val_acc'])\n",
    "plt.title(\"Validation Accuracy\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run():\n",
    "    x = torch.randn(1000, 1, 28, 28).to(device)\n",
    "    out = model(x, steps=100)\n",
    "\n",
    "%timeit run()\n",
    "for p in model.parameters():\n",
    "    p.requires_grad = False\n",
    "%timeit run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Current Accuracy and Error Rate\n",
    "acc = accuracy(model, val_dataset, steps=0)\n",
    "error = 100 * (1 - acc)\n",
    "print(f'Current Val Acc: {acc} | error_rate: {error:0.2f} %')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Shows statistics over multiple models. models = [model1, model2, ...]\n",
    "\n",
    "\n",
    "# train_vfes = torch.tensor([stats[i]['train_vfe'][-1] for i in range(num_models)])\n",
    "# val_vfes = torch.tensor([stats[i]['val_vfe'][-1] for i in range(num_models)])\n",
    "# val_accs = torch.tensor([stats[i]['val_acc'][-1] for i in range(num_models)])\n",
    "\n",
    "# # Show statistics across models, std is nan if num_models = 1\n",
    "# print(f\"Tra VFE - mean: {train_vfes.mean():.3f} | std: {train_vfes.std():.3f} | min: {train_vfes.min():.3f} | max: {train_vfes.max():.3f}\")\n",
    "# print(f\"Val VFE - mean: {val_vfes.mean():.3f} | std: {val_vfes.std():.3f} | min: {val_vfes.min():.3f} | max: {val_vfes.max():.3f}\")\n",
    "# print(f\"Val Acc - mean: {val_accs.mean():.3f} | std: {val_accs.std():.3f} | min: {val_accs.min():.3f} | max: {val_accs.max():.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pclib.utils.functional import format_y\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=512, shuffle=True)\n",
    "images, y = next(iter(train_loader))\n",
    "x = images\n",
    "y = format_y(y, 10)\n",
    "\n",
    "track_vfe(model, x, y, 100, plot_Es=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert isinstance(model.layers[0], PrecisionWeighted), \"This cell only works for PrecisionWeighted layers\"\n",
    "# show diag of weight_var matrix as 28x28 image for layer 0\n",
    "model = model\n",
    "layer = 0\n",
    "weight_var = model.layers[0].weight_var.detach().cpu().numpy()\n",
    "# weight_var = model.layers[-1].weight_var.diag().reshape(28,28).detach().cpu().numpy()\n",
    "plt.imshow(weight_var, cmap='gray')\n",
    "plt.colorbar()\n",
    "plt.show()\n",
    "weight_var.min(), weight_var.max()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
