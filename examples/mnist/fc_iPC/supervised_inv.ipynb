{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import os\n",
    "\n",
    "from pclib.nn.models import FCPCN\n",
    "from pclib.optim.eval import track_vfe, accuracy\n",
    "from pclib.utils.functional import format_y, identity, shrinkage\n",
    "from pclib.utils.customdataset import PreloadedDataset\n",
    "from tqdm import tqdm\n",
    "from pclib.optim.train import val_pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seed = 42\n",
    "# For reproducibility\n",
    "os.environ['CUBLAS_WORKSPACE_CONFIG'] = ':4096:8'\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                        \r"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(seed)\n",
    "\n",
    "# Scale img transform\n",
    "class Scale(object):\n",
    "    def __init__(self, scale_factor):\n",
    "        self.scale_factor = scale_factor\n",
    "\n",
    "    def __call__(self, img):\n",
    "        return img * self.scale_factor + 0.5 * (1 - self.scale_factor)\n",
    "\n",
    "# Inverse Logistic Sigmoid Transform\n",
    "class InvLogit(object):\n",
    "    def __call__(self, x):\n",
    "        return torch.log(x / (1 - x))\n",
    "\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    Scale(0.9),\n",
    "    InvLogit(),\n",
    "    # transforms.RandomAffine(degrees=10, translate=(0.1, 0.1), scale=(0.9, 1.1)),                                \n",
    "    # transforms.Normalize((0.1307,), (0.3081,)),\n",
    "])\n",
    "\n",
    "val_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    Scale(0.9),\n",
    "    InvLogit(),\n",
    "    # transforms.Normalize((0.1307,), (0.3081,)),\n",
    "])\n",
    "\n",
    "dataset = datasets.MNIST('../Datasets/', train=True, download=False, transform=transforms.ToTensor())\n",
    "\n",
    "VAL_RATIO = 0.2\n",
    "val_len = int(len(dataset) * VAL_RATIO)\n",
    "train_len = len(dataset) - val_len\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_len, val_len])\n",
    "train_dataset = PreloadedDataset.from_dataset(train_dataset, train_transform, device)\n",
    "val_dataset = PreloadedDataset.from_dataset(val_dataset, val_transform, device)\n",
    "INPUT_SHAPE = 784\n",
    "NUM_CLASSES = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1500x500 with 10 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max value:  tensor(2.9444, device='cuda:0')\n",
      "Min value:  tensor(-2.9444, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "# Show some images\n",
    "fig, axes = plt.subplots(1, 10, figsize=(15, 5))\n",
    "for i, ax in enumerate(axes):\n",
    "    img, label = train_dataset[i]\n",
    "    ax.imshow(img.squeeze().cpu(), cmap='gray')\n",
    "    ax.axis('off')\n",
    "plt.show()\n",
    "\n",
    "# print max min values\n",
    "print('Max value: ', train_dataset.transformed_images.max())\n",
    "print('Min value: ', train_dataset.transformed_images.min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_stats = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(\n",
    "        model,\n",
    "        train_dataset,\n",
    "        val_dataset,\n",
    "        BATCH_SIZE,\n",
    "        NUM_EPOCHS,\n",
    "        optimiser,\n",
    "        scheduler,\n",
    "        stats,\n",
    "        norm_grads=False,\n",
    "        neg_cnst=None,\n",
    "        cd=None,\n",
    "        eval_every=100,\n",
    "):\n",
    "    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "    pos_states = [model.init_state(images.flatten(1), F.one_hot(labels, 10)) for images, labels in train_loader]\n",
    "    neg_states = [model.init_state(images.flatten(1), F.one_hot(labels, 10)) for images, labels in train_loader]\n",
    "\n",
    "    model.train()\n",
    "    gamma = torch.ones(pos_states[0][0]['x'].shape[0], device=device) * model.gamma\n",
    "    cd_gamma = torch.ones(pos_states[0][0]['x'].shape[0], device=device) * model.gamma\n",
    "\n",
    "    loop = tqdm(range(NUM_EPOCHS), leave=False)\n",
    "    for epoch in loop:\n",
    "\n",
    "        if stats['epoch'] > 0:\n",
    "            postfix = {'steps': stats['epoch'], 'posVFE': stats['posVfe'][-1], 'valAcc': stats['valAcc'][-1], 'valVFE': stats['valVfe'][-1]}\n",
    "            if neg_cnst:\n",
    "                postfix['negVFE'] = stats['negVfe'][-1]\n",
    "                postfix['negMSE'] = stats['negMse'][-1]\n",
    "            if cd:\n",
    "                postfix['cdVFE'] = stats['cdVfe'][-1]\n",
    "            loop.set_postfix(postfix)\n",
    "        \n",
    "        epoch_stats = {'posVfe': [], 'negVfe': [], 'cdVfe': [], 'negMse': []}\n",
    "        \n",
    "        for i in range(len(train_loader)):\n",
    "\n",
    "            # Update particles and calculate new VFE\n",
    "            model.step(pos_states[i], gamma, pin_obs=True, pin_target=True)\n",
    "            pos_vfe = model.vfe(pos_states[i], normalise=norm_grads)\n",
    "            loss = pos_vfe.clone()\n",
    "\n",
    "            # Same For Negative particles\n",
    "            if neg_cnst:\n",
    "                neg_states[i][-1]['x'] = pos_states[i][-1]['x']\n",
    "                # for _ in range(20):\n",
    "                model.step(neg_states[i], gamma, pin_target=True)\n",
    "                neg_vfe = model.vfe(neg_states[i], normalise=norm_grads)\n",
    "                reconstruction = neg_states[i][0]['x']\n",
    "                neg_mse = F.mse_loss(reconstruction, pos_states[i][0]['x'])\n",
    "                loss -= neg_cnst*neg_vfe\n",
    "            if cd:\n",
    "                cd_state = [{k: v.clone() for k, v in state_l.items()} for state_l in pos_states[i]]\n",
    "                # for _ in range(cd):\n",
    "                #     model.step(cd_state, cd_gamma, pin_obs=False)\n",
    "                \n",
    "                out, _ = model.reconstruct(cd_state[0]['x'], steps=cd)\n",
    "                cd_state[0]['x'] = out\n",
    "\n",
    "                cd_vfe = model.vfe(cd_state, normalise=norm_grads)\n",
    "                loss += -0.75 * cd_vfe\n",
    "\n",
    "            optimiser.zero_grad()\n",
    "            loss.backward()\n",
    "            optimiser.step()\n",
    "            if model.has_top:\n",
    "                model.top.weight.data = model.top.weight.data - torch.diag(model.top.weight.data.diag())\n",
    "\n",
    "            # if norm_grads, recalculate VFE WITHOUT normalisation for plotting (otherwise it will be constant)\n",
    "            if norm_grads:\n",
    "                pos_vfe = model.vfe(pos_states[i], normalise=False)\n",
    "                if neg_cnst:\n",
    "                    neg_vfe = model.vfe(neg_states[i], normalise=False)\n",
    "                if cd:\n",
    "                    cd_vfe = model.vfe(cd_state, normalise=False)\n",
    "\n",
    "            epoch_stats['posVfe'].append(pos_vfe.item())\n",
    "            if neg_cnst:\n",
    "                epoch_stats['negVfe'].append(neg_vfe.item())\n",
    "                epoch_stats['negMse'].append(neg_mse.item())\n",
    "            if cd:\n",
    "                epoch_stats['cdVfe'].append(cd_vfe.item())\n",
    "            \n",
    "        if epoch % eval_every == 0:\n",
    "            val_stats = val_pass(model, None, val_loader)\n",
    "            stats['valAcc'].append(val_stats['acc'].item())\n",
    "            stats['valVfe'].append(val_stats['vfe'].item())\n",
    "\n",
    "        pos_vfe = sum(epoch_stats['posVfe']) / len(epoch_stats['posVfe'])\n",
    "        if pos_vfe < 0:\n",
    "            raise ValueError(f'epoch VFE is negative: {pos_vfe}')\n",
    "        stats['posVfe'].append(sum(epoch_stats['posVfe']) / len(epoch_stats['posVfe']))\n",
    "        if neg_cnst:\n",
    "            stats['negVfe'].append(sum(epoch_stats['negVfe']) / len(epoch_stats['negVfe'])) \n",
    "            stats['negMse'].append(sum(epoch_stats['negMse']) / len(epoch_stats['negMse']))\n",
    "        if cd:\n",
    "            stats['cdVfe'].append(sum(epoch_stats['cdVfe']) / len(epoch_stats['cdVfe']))\n",
    "\n",
    "        # Update learning rate\n",
    "        if scheduler is not None:\n",
    "            scheduler.step(stats['posVfe'][-1])\n",
    "        \n",
    "        stats['epoch'] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(seed)\n",
    "\n",
    "model = FCPCN(\n",
    "    sizes=[10, 600, 600, 784],\n",
    "    precisions=[1.0, 1.0, 1.0, 1.0],\n",
    "    bias=True, \n",
    "    symmetric=True, \n",
    "    actv_fn=F.sigmoid,\n",
    "    # actv_fn=shrinkage,\n",
    "    steps=20,\n",
    "    gamma=0.1,\n",
    "    x_decay=0.0,\n",
    "    inverted = True,\n",
    ").to(device)\n",
    "stats = {\n",
    "    'epoch': 0,\n",
    "    'posVfe': [],\n",
    "    'negVfe': [],\n",
    "    'negMse': [],\n",
    "    'cdVfe': [],\n",
    "    'valVfe': [],\n",
    "    'valAcc': [],\n",
    "}\n",
    "optimiser = torch.optim.AdamW(model.parameters(), lr=0.001, weight_decay=0.02)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimiser, patience=500, factor=0.5, verbose=True, threshold=1e-4, min_lr=1e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|▉         | 954/10000 [4:36:49<39:41:51, 15.80s/it, steps=954, posVFE=533, valAcc=0.107, valVFE=5.9]  "
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 20\n",
    "NUM_EPOCHS = 10000\n",
    "stats = train(\n",
    "    model,\n",
    "    train_dataset,\n",
    "    val_dataset,\n",
    "    BATCH_SIZE,\n",
    "    NUM_EPOCHS,\n",
    "    optimiser,\n",
    "    # scheduler,\n",
    "    None,\n",
    "    stats,\n",
    "    eval_every=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_stats['basic'] = stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[40], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m all_stats\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m----> 2\u001b[0m     plt\u001b[38;5;241m.\u001b[39mplot(\u001b[43mv\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtrainVfe\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m, label\u001b[38;5;241m=\u001b[39mk)\n\u001b[0;32m      3\u001b[0m     plt\u001b[38;5;241m.\u001b[39mplot(v[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnegVfe\u001b[39m\u001b[38;5;124m'\u001b[39m], label\u001b[38;5;241m=\u001b[39mk\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m (neg)\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      4\u001b[0m plt\u001b[38;5;241m.\u001b[39mlegend()\n",
      "\u001b[1;31mTypeError\u001b[0m: 'NoneType' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "for k, v in all_stats.items():\n",
    "    plt.plot(v['trainVfe'], label=k)\n",
    "    plt.plot(v['negVfe'], label=k+' (neg)')\n",
    "plt.legend()\n",
    "plt.yscale('log')\n",
    "plt.title('TrainVFE')\n",
    "plt.show()\n",
    "\n",
    "for k, v in all_stats.items():\n",
    "    plt.plot(v['valVfe'], label=k)\n",
    "plt.legend()\n",
    "plt.title('valVFE')\n",
    "plt.show()\n",
    "\n",
    "for k, v in all_stats.items():\n",
    "    plt.plot(v['valAcc'], label=k)\n",
    "plt.legend()\n",
    "plt.title('ValAcc')\n",
    "# plt.yscale('log')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
