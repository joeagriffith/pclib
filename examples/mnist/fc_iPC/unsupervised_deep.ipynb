{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import os\n",
    "\n",
    "from pclib.nn.models import FCDPCN, Classifier, FCPCN\n",
    "from pclib.nn.layers import DFC\n",
    "from pclib.optim.eval import track_vfe, accuracy\n",
    "from pclib.utils.functional import format_y, identity, shrinkage\n",
    "from pclib.utils.customdataset import PreloadedDataset\n",
    "from tqdm import tqdm\n",
    "from pclib.optim.train_DeepiPC import train_iPC, val_pass, train_iPC_classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seed = 42\n",
    "# For reproducibility\n",
    "# os.environ['CUBLAS_WORKSPACE_CONFIG'] = ':4096:8'\n",
    "torch.backends.cudnn.deterministic = True\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                        \r"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(seed)\n",
    "\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    # transforms.RandomAffine(degrees=10, translate=(0.1, 0.1), scale=(0.9, 1.1)),                                \n",
    "    # transforms.Normalize((0.1307,), (0.3081,)),\n",
    "])\n",
    "\n",
    "val_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    # transforms.Normalize((0.1307,), (0.3081,)),\n",
    "])\n",
    "\n",
    "dataset = datasets.MNIST('../Datasets/', train=True, download=False, transform=transforms.ToTensor())\n",
    "\n",
    "VAL_RATIO = 0.2\n",
    "val_len = int(len(dataset) * VAL_RATIO)\n",
    "train_len = len(dataset) - val_len\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_len, val_len])\n",
    "train_dataset = PreloadedDataset.from_dataset(train_dataset, train_transform, device)\n",
    "val_dataset = PreloadedDataset.from_dataset(val_dataset, val_transform, device)\n",
    "\n",
    "INPUT_SHAPE = 784\n",
    "NUM_CLASSES = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1500x500 with 10 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max value:  tensor(1., device='cuda:0')\n",
      "Min value:  tensor(0., device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "# Show some images\n",
    "fig, axes = plt.subplots(1, 10, figsize=(15, 5))\n",
    "for i, ax in enumerate(axes):\n",
    "    img, label = train_dataset[i]\n",
    "    ax.imshow(img.squeeze().cpu(), cmap='gray')\n",
    "    ax.axis('off')\n",
    "plt.show()\n",
    "\n",
    "# print max min values\n",
    "print('Max value: ', train_dataset.transformed_images.max())\n",
    "print('Min value: ', train_dataset.transformed_images.min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial VFE: 45.1286, Final VFE: 42.0816\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1d44d928650>]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAiwAAAGdCAYAAAAxCSikAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8pXeV/AAAACXBIWXMAAA9hAAAPYQGoP6dpAAAzXklEQVR4nO3de3RV9Z3//9fJOclJyBUSyEUDREBSGlIEKsUy1vklC4JZCtRvO2rsUsdVlKEVtBVKK06t2KS0nTJMtTL8RhxXqcxohbF+FX4UC+qIMQYiUDUCggRDkgLmnFxIyOXz+yPk4JEQcq77JDwfq3uZ7L2z8z4fV81rfW7bZowxAgAAiGBRVhcAAABwKQQWAAAQ8QgsAAAg4hFYAABAxCOwAACAiEdgAQAAEY/AAgAAIh6BBQAARDyH1QUEQ3d3t2pra5WYmCibzWZ1OQAAYACMMWpqalJWVpaiovrvQxkSgaW2tlbZ2dlWlwEAAPxQU1OjK6+8st97hkRgSUxMlNTzgZOSkiyuBgAADITb7VZ2drbn73h/hkRg6R0GSkpKIrAAADDIDGQ6B5NuAQBAxCOwAACAiEdgAQAAEY/AAgAAIh6BBQAARDwCCwAAiHgEFgAAEPEILAAAIOIRWAAAQMQjsAAAgIhHYAEAABGPwAIAACLekHj5YajUudq04X+PSDZpxdwvWV0OAACXLXpY+tHc3ql1r3+sP5Qfs7oUAAAuawSWfgwfFi1JamrrVGdXt8XVAABw+SKw9CM5LtrztetMh4WVAABweSOw9MNhj1JibM80n0YCCwAAliGwXMLwYTGSpMbWsxZXAgDA5YvAcgkp5+axNLbSwwIAgFUILJeQcq6H5TMCCwAAliGwXELvHJbmNgILAABWIbBcQqLzXGBp77S4EgAALl8ElktIOBdYmggsAABYJqDAUlZWJpvNpqVLl3rO3XDDDbLZbF7Hfffd1+9zjDF65JFHlJmZqbi4OBUWFurgwYOBlBY0CZ4hIQILAABW8TuwVFRUaN26dcrPz7/g2ne/+12dOHHCc6xevbrfZ61evVpr167VU089pfLycsXHx2vOnDlqa2vzt7ygSWBICAAAy/kVWJqbm1VSUqL169dr+PDhF1wfNmyYMjIyPEdSUtJFn2WM0Zo1a/Twww9r3rx5ys/P17PPPqva2lpt2bLFn/KCKpEeFgAALOdXYFm8eLGKi4tVWFjY5/WNGzcqLS1NeXl5WrFihVpbWy/6rCNHjqiurs7rWcnJyZoxY4Z2797tT3lBleA89z4helgAALCMw9cf2LRpk/bs2aOKioo+r99+++0aM2aMsrKytG/fPi1fvlzV1dV68cUX+7y/rq5OkpSenu51Pj093XPti9rb29Xe3u753u12+/oxBow5LAAAWM+nwFJTU6MlS5Zo+/btio2N7fOehQsXer6ePHmyMjMzVVBQoMOHD2vcuHGBVXtOaWmpHn300aA861KYwwIAgPV8GhKqrKxUQ0ODpk6dKofDIYfDoV27dmnt2rVyOBzq6uq64GdmzJghSTp06FCfz8zIyJAk1dfXe52vr6/3XPuiFStWyOVyeY6amhpfPoZPPHNYCCwAAFjGpx6WgoIC7d+/3+vc3XffrdzcXC1fvlx2u/2Cn6mqqpIkZWZm9vnMnJwcZWRkaMeOHZoyZYqkniGe8vJyLVq0qM+fcTqdcjqdvpTuN3pYAACwnk+BJTExUXl5eV7n4uPjlZqaqry8PB0+fFh/+MMfdOONNyo1NVX79u3TAw88oOuvv95r+XNubq5KS0u1YMECzz4uq1at0oQJE5STk6OVK1cqKytL8+fPD8qHDETvHJaznd1q7+yS03FhKAMAAKHl86Tb/sTExOjPf/6z1qxZo5aWFmVnZ+uWW27Rww8/7HVfdXW1XC6X5/tly5appaVFCxcuVGNjo2bNmqWtW7dedJ5MOMXHnG+ilnYCCwAAVrAZY4zVRQTK7XYrOTlZLper3z1f/DXpka1qPdul1x/6e41OHRb05wMAcDny5e837xIagPPvE+KNzQAAWIHAMgDsxQIAgLUILAOQyEohAAAsRWAZgAT2YgEAwFIElgHwzGFhSAgAAEsQWAag9wWI9LAAAGANAssAJDLpFgAASxFYBoDt+QEAsBaBZQCYdAsAgLUILAPg6WFhSAgAAEsQWAaAISEAAKxFYBmA81vzE1gAALACgWUAzm/Nz7uEAACwAoFlABgSAgDAWgSWAWAfFgAArEVgGYDeHpaWs13q6jYWVwMAwOWHwDIAvXNYJKnlLL0sAACEG4FlAJwOu2LsPU3FsBAAAOFHYBkgdrsFAMA6BJYBYqUQAADWIbAMENvzAwBgHQLLADEkBACAdQgsA0QPCwAA1iGwDBDvEwIAwDoElgFKYLdbAAAsQ2AZoETPKiFegAgAQLgRWAaIZc0AAFiHwDJAvUNCboaEAAAIOwLLACXGRktiDgsAAFYgsAxQ4rkelqY25rAAABBuBJYB6p1020QPCwAAYUdgGaDeISECCwAA4UdgGSCGhAAAsA6BZYB6A0vL2S51dRuLqwEA4PJCYBmg3iEhiZVCAACEG4FlgGIcUXI6epqrid1uAQAIKwKLD87PY6GHBQCAcCKw+ICVQgAAWIPA4gNWCgEAYA0Ciw8YEgIAwBoEFh8kOnuHhOhhAQAgnAgsPkjkjc0AAFiCwOKDBIaEAACwBIHFB72rhJrZhwUAgLAisPggiR4WAAAsEVBgKSsrk81m09KlSy+4ZozR3LlzZbPZtGXLln6fc9ddd8lms3kdRUVFgZQWEqwSAgDAGg5/f7CiokLr1q1Tfn5+n9fXrFkjm8024OcVFRVpw4YNnu+dTqe/pYXM+Y3jGBICACCc/OphaW5uVklJidavX6/hw4dfcL2qqkq//vWv9fTTTw/4mU6nUxkZGZ6jr+dajR4WAACs4VdgWbx4sYqLi1VYWHjBtdbWVt1+++164oknlJGRMeBn7ty5U6NGjdLEiRO1aNEinTp16qL3tre3y+12ex3hwNb8AABYw+choU2bNmnPnj2qqKjo8/oDDzyg6667TvPmzRvwM4uKivTNb35TOTk5Onz4sH784x9r7ty52r17t+x2+wX3l5aW6tFHH/W19IAlOHv3YWFICACAcPIpsNTU1GjJkiXavn27YmNjL7j+0ksv6bXXXtPevXt9KuLWW2/1fD158mTl5+dr3Lhx2rlzpwoKCi64f8WKFXrwwQc937vdbmVnZ/v0O/3Ru0qoub1Txhif5ugAAAD/+TQkVFlZqYaGBk2dOlUOh0MOh0O7du3S2rVr5XA4tH37dh0+fFgpKSme65J0yy236IYbbhjw77nqqquUlpamQ4cO9Xnd6XQqKSnJ6wiH3iEhY6SWs11h+Z0AAMDHHpaCggLt37/f69zdd9+t3NxcLV++XGlpabr33nu9rk+ePFm/+c1vdNNNNw349xw/flynTp1SZmamL+WFXGx0lBxRNnV2GzW1dXiGiAAAQGj59Bc3MTFReXl5Xufi4+OVmprqOd/XRNvRo0crJyfH831ubq5KS0u1YMECNTc369FHH9Utt9yijIwMHT58WMuWLdP48eM1Z84cfz5TyNhsNiXGOvRZa4ea2jqVmWx1RQAAXB4s2em2urpaLpdLkmS327Vv3z7dfPPNuvrqq3XPPfdo2rRpeuONN9iLBQAASApg47heO3fu7Pe6Mabfc3Fxcdq2bVugZYTN+ZVCLG0GACBceJeQj9g8DgCA8COw+IghIQAAwo/A4iPPXiz0sAAAEDYEFh8xJAQAQPgRWHzEkBAAAOFHYPERPSwAAIQfgcVHCbEsawYAINwILD5iSAgAgPAjsPiIISEAAMKPwOKj3mXNTe30sAAAEC4EFh+dHxKihwUAgHAhsPio911CzW2dfb4nCQAABB+BxUe9c1g6u43aO7strgYAgMsDgcVH8TEO2Ww9X7tZKQQAQFgQWHwUFWVTQgwrhQAACCcCix8SeQEiAABhRWDxAyuFAAAILwKLHxI8m8cxhwUAgHAgsPjBs9ttOz0sAACEA4HFDwwJAQAQXgQWP/RuHseQEAAA4UFg8UMSq4QAAAgrAosfeGMzAADhRWDxg2dIiDc2AwAQFgQWPzDpFgCA8CKw+CGBISEAAMKKwOKHRDaOAwAgrAgsfkg6NyTUzMZxAACEBYHFD+f3YSGwAAAQDgQWP/QOCbWe7VJnV7fF1QAAMPQRWPzQO+lWklrauyysBACAywOBxQ9Oh10xjp6mczPxFgCAkCOw+MmzPT8TbwEACDkCi5/YPA4AgPAhsPiJNzYDABA+BBY/JTIkBABA2BBY/NQbWNwMCQEAEHIEFj8lOHvnsDAkBABAqBFY/OQZEqKHBQCAkCOw+CmJNzYDABA2BBY/JfDGZgAAwobA4qdE3tgMAEDYEFj8xCohAADCh8Dip/MbxxFYAAAItYACS1lZmWw2m5YuXXrBNWOM5s6dK5vNpi1btvT7HGOMHnnkEWVmZiouLk6FhYU6ePBgIKWF3PkhIeawAAAQan4HloqKCq1bt075+fl9Xl+zZo1sNtuAnrV69WqtXbtWTz31lMrLyxUfH685c+aora3N3/JCLpFVQgAAhI1fgaW5uVklJSVav369hg8ffsH1qqoq/frXv9bTTz99yWcZY7RmzRo9/PDDmjdvnvLz8/Xss8+qtrb2kj0zVvp8YDHGWFwNAABDm1+BZfHixSouLlZhYeEF11pbW3X77bfriSeeUEZGxiWfdeTIEdXV1Xk9Kzk5WTNmzNDu3bv7/Jn29na53W6vI9x6h4S6uo3aOrrD/vsBALicOHz9gU2bNmnPnj2qqKjo8/oDDzyg6667TvPmzRvQ8+rq6iRJ6enpXufT09M9176otLRUjz76qA9VB9+waLtsNsmYnr1Y4mLsltYDAMBQ5lMPS01NjZYsWaKNGzcqNjb2gusvvfSSXnvtNa1ZsyZY9fVpxYoVcrlcnqOmpiakv68vUVE2z0ohljYDABBaPgWWyspKNTQ0aOrUqXI4HHI4HNq1a5fWrl0rh8Oh7du36/Dhw0pJSfFcl6RbbrlFN9xwQ5/P7B02qq+v9zpfX19/0SElp9OppKQkr8MKSWweBwBAWPg0JFRQUKD9+/d7nbv77ruVm5ur5cuXKy0tTffee6/X9cmTJ+s3v/mNbrrppj6fmZOTo4yMDO3YsUNTpkyRJLndbpWXl2vRokW+lBd25/diYWkzAACh5FNgSUxMVF5ente5+Ph4paames731SsyevRo5eTkeL7Pzc1VaWmpFixY4NnHZdWqVZowYYJycnK0cuVKZWVlaf78+X58pPDhjc0AAISHz5Nug6G6uloul8vz/bJly9TS0qKFCxeqsbFRs2bN0tatW/ucJxNJ2IsFAIDwCDiw7Ny5s9/rfe1R8sVzNptNP/vZz/Szn/0s0HLCKuHcHBY3Q0IAAIQU7xIKgGdIiEm3AACEFIElAAwJAQAQHgSWACSySggAgLAgsAQgkX1YAAAICwJLABgSAgAgPAgsAWBrfgAAwoPAEgDPkBBzWAAACCkCSwAYEgIAIDwILAEgsAAAEB4ElgD0Dgmd6ehSZ1e3xdUAADB0EVgC0DvpVmJpMwAAoURgCUCMI0pOR08TMiwEAEDoEFgC1DssRGABACB0CCwBOj/xlqXNAACECoElQKwUAgAg9AgsAeoNLEy6BQAgdAgsAUrgjc0AAIQcgSVAnkm39LAAABAyBJYAMYcFAIDQI7AEKJEhIQAAQo7AEqDzb2ymhwUAgFAhsASIISEAAEKPwBKgBAILAAAhR2AJEKuEAAAIPQJLgNiaHwCA0COwBOj8KiF6WAAACBUCS4CS4nrf1twhY4zF1QAAMDQRWAKUdG4OS7fhfUIAAIQKgSVAsdFRirH3NKPrDPNYAAAIBQJLgGw2m2dYyH2GHhYAAEKBwBIESXE9E2/pYQEAIDQILEGQfK6HhcACAEBoEFiCINkzJERgAQAgFAgsQeAJLGweBwBASBBYgqB3aTNDQgAAhAaBJQiYwwIAQGgRWIKAOSwAAIQWgSUIWNYMAEBoEViCgCEhAABCi8ASBJ6dbnljMwAAIUFgCQJ6WAAACC0CSxCwrBkAgNAisARB8rCewHK2s1ttHV0WVwMAwNBDYAmChBiHomw9X7O0GQCA4AsosJSVlclms2np0qWec/fee6/GjRunuLg4jRw5UvPmzdOHH37Y73Puuusu2Ww2r6OoqCiQ0sIqKsrmmXjLsBAAAMHnd2CpqKjQunXrlJ+f73V+2rRp2rBhgz744ANt27ZNxhjNnj1bXV39D5UUFRXpxIkTnuO5557ztzRLMI8FAIDQcfjzQ83NzSopKdH69eu1atUqr2sLFy70fD127FitWrVKX/nKV3T06FGNGzfuos90Op3KyMjwp5yIwAsQAQAIHb96WBYvXqzi4mIVFhb2e19LS4s2bNignJwcZWdn93vvzp07NWrUKE2cOFGLFi3SqVOnLnpve3u73G6312E1ljYDABA6PgeWTZs2ac+ePSotLb3oPU8++aQSEhKUkJCgV199Vdu3b1dMTMxF7y8qKtKzzz6rHTt26Be/+IV27dqluXPnXnQYqbS0VMnJyZ7jUmEoHDzb87cSWAAACDafAktNTY2WLFmijRs3KjY29qL3lZSUaO/evdq1a5euvvpqffvb31ZbW9tF77/11lt18803a/LkyZo/f75efvllVVRUaOfOnX3ev2LFCrlcLs9RU1Pjy8cIiWR2uwUAIGR8CiyVlZVqaGjQ1KlT5XA45HA4tGvXLq1du1YOh8PTI5KcnKwJEybo+uuv1wsvvKAPP/xQmzdvHvDvueqqq5SWlqZDhw71ed3pdCopKcnrsBqrhAAACB2fJt0WFBRo//79Xufuvvtu5ebmavny5bLb7Rf8jDFGxhi1t7cP+PccP35cp06dUmZmpi/lWYo5LAAAhI5PPSyJiYnKy8vzOuLj45Wamqq8vDx9/PHHKi0tVWVlpY4dO6a33npL3/rWtxQXF6cbb7zR85zc3FxPj0tzc7Meeughvf322zp69Kh27NihefPmafz48ZozZ05wP20IsawZAIDQCepOt7GxsXrjjTd04403avz48fqHf/gHJSYm6q233tKoUaM891VXV8vlckmS7Ha79u3bp5tvvllXX3217rnnHk2bNk1vvPGGnE5nMMsLKXpYAAAIHb/2Yfm8z0+MzcrK0iuvvHLJnzHGeL6Oi4vTtm3bAi3Dcinn3ifEKiEAAIKPdwkFyfBhPcu2P2s9a3ElAAAMPQSWIOntYWls7fDqQQIAAIEjsARJbw/L2a5utZ7t/71JAADANwSWIBkWY1eMvac5GRYCACC4CCxBYrPZvIaFAABA8BBYgoiJtwAAhAaBJYh6e1g+o4cFAICgIrAE0Yj4nh6WRnpYAAAIKgJLEKX0Dgm10MMCAEAwEViCaLhnSIgeFgAAgonAEkRMugUAIDQILEHEpFsAAEKDwBJEvT0sTLoFACC4CCxBNDyeOSwAAIQCgSWIelcJNbJKCACAoCKwBFHvkFBTe6c6urotrgYAgKGDwBJEyXHRstl6vuZ9QgAABA+BJYjsUTYlxfa+AJF5LAAABAuBJciGs7QZAICgI7AEWQqbxwEAEHQEliDr7WFhSAgAgOAhsATZ+e35GRICACBYCCxBxpAQAADBR2AJstSEnsByupnAAgBAsBBYgiw1viewnGohsAAAECwEliBLTXBKkk41t1tcCQAAQweBJch6h4ROMiQEAEDQEFiCbOS5HpaTze0yxlhcDQAAQwOBJch6e1jaO7vVcrbL4moAABgaCCxBNizGobhouyTmsQAAECwElhBgHgsAAMFFYAmBNFYKAQAQVASWEEijhwUAgKAisIRAajw9LAAABBOBJQR657Cw2y0AAMFBYAmB1M/txQIAAAJHYAmB3jksp5jDAgBAUBBYQiCNHhYAAIKKwBICzGEBACC4CCwh0LtK6LPWs+rs6ra4GgAABj8CSwiMiI+RzSYZI33W2mF1OQAADHoElhCwR9k0YljvsBDzWAAACBSBJUQ87xNqYh4LAACBIrCESO88FlYKAQAQuIACS1lZmWw2m5YuXeo5d++992rcuHGKi4vTyJEjNW/ePH344Yf9PscYo0ceeUSZmZmKi4tTYWGhDh48GEhplhuV1BNY/tZEYAEAIFB+B5aKigqtW7dO+fn5XuenTZumDRs26IMPPtC2bdtkjNHs2bPV1dV10WetXr1aa9eu1VNPPaXy8nLFx8drzpw5amtr87c8y6UnxUqS6t2D9zMAABAp/Aoszc3NKikp0fr16zV8+HCvawsXLtT111+vsWPHaurUqVq1apVqamp09OjRPp9ljNGaNWv08MMPa968ecrPz9ezzz6r2tpabdmyxZ/yIsKoxJ4eljoCCwAAAfMrsCxevFjFxcUqLCzs976WlhZt2LBBOTk5ys7O7vOeI0eOqK6uzutZycnJmjFjhnbv3t3nz7S3t8vtdnsdkaa3h6XBzZAQAACB8jmwbNq0SXv27FFpaelF73nyySeVkJCghIQEvfrqq9q+fbtiYmL6vLeurk6SlJ6e7nU+PT3dc+2LSktLlZyc7DkuFoaslJF8bkioiR4WAAAC5VNgqamp0ZIlS7Rx40bFxsZe9L6SkhLt3btXu3bt0tVXX61vf/vbQZ2PsmLFCrlcLs9RU1MTtGcHS3ri+TksxhiLqwEAYHBz+HJzZWWlGhoaNHXqVM+5rq4uvf766/rtb3+r9vZ22e12T8/HhAkT9LWvfU3Dhw/X5s2bddttt13wzIyMDElSfX29MjMzPefr6+s1ZcqUPutwOp1yOp2+lB52vauE2jq65W7rVHJctMUVAQAwePnUw1JQUKD9+/erqqrKc0yfPl0lJSWqqqqS3W6/4GeMMTLGqL2977kcOTk5ysjI0I4dOzzn3G63ysvLNXPmTB8/TuSIjbZ7QgorhQAACIxPPSyJiYnKy8vzOhcfH6/U1FTl5eXp448/1n/9139p9uzZGjlypI4fP66ysjLFxcXpxhtv9PxMbm6uSktLtWDBAs8+LqtWrdKECROUk5OjlStXKisrS/Pnzw/Kh7RKRlKsXGc6VO9u09XpiVaXAwDAoOVTYLmU2NhYvfHGG1qzZo0+++wzpaen6/rrr9dbb72lUaNGee6rrq6Wy+XyfL9s2TK1tLRo4cKFamxs1KxZs7R169Z+58kMBqOSnKqub1I9K4UAAAiIzQyBGaFut1vJyclyuVxKSkqyuhyPHz7/nl6oPK6H5kzU4r8fb3U5AABEFF/+fvMuoRBKPzfxljksAAAEhsASQmzPDwBAcBBYQuh8YGEOCwAAgSCwhND57fnpYQEAIBAElhDqncPS0NSu7u5BP7cZAADLEFhCKC3BKZtN6uw2OtVy1upyAAAYtAgsIRRtj1JaQk8vS52LYSEAAPxFYAmxrJQ4SdKnjWcsrgQAgMGLwBJiV6T0TLytJbAAAOA3AkuIXUEPCwAAASOwhJgnsHxGYAEAwF8ElhC7YvgwSVKti8ACAIC/CCwhlnVuDgs9LAAA+I/AEmJXpvT0sJxqOaszZ7ssrgYAgMGJwBJiSXEOJTgdkhgWAgDAXwSWELPZbAwLAQAQIAJLGLC0GQCAwBBYwuCK4SxtBgAgEASWMMg+t7T52OlWiysBAGBwIrCEwZjUnsDyCYEFAAC/EFjCYExqvCTpk1MtFlcCAMDgRGAJg9EjenpYGls75DrTYXE1AAAMPgSWMIh3OpSW4JQkHTvFsBAAAL4isITJWM88FoaFAADwFYElTEb3BhZ6WAAA8BmBJUzGjGDiLQAA/iKwhMkYelgAAPAbgSVMCCwAAPiPwBImV41MkCTVudvU3N5pcTUAAAwuBJYwSY6L1sjEnqXNhxuaLa4GAIDBhcASRuNG9ky8Pfw3AgsAAL4gsITRuHPDQofoYQEAwCcEljAaP6onsNDDAgCAbwgsYUQPCwAA/iGwhFFvD8snp1rV0dVtcTUAAAweBJYwykyO1bAYuzq7DfuxAADgAwJLGNlsNk8vy0f1TRZXAwDA4EFgCbPcjERJ0gcn3BZXAgDA4EFgCbMvZSZJkj44QQ8LAAADRWAJs/OBhR4WAAAGisASZl/K6AksnzaeketMh8XVAAAwOBBYwix5WLSykmMlSR/SywIAwIAQWCzAsBAAAL4hsFigN7C8T2ABAGBAAgosZWVlstlsWrp0qSTp9OnT+v73v6+JEycqLi5Oo0eP1v333y+Xy9Xvc+666y7ZbDavo6ioKJDSItrkK5MlSfuO998uAACgh8PfH6yoqNC6deuUn5/vOVdbW6va2lr96le/0qRJk/TJJ5/ovvvuU21trV544YV+n1dUVKQNGzZ4vnc6nf6WFvGmZKdI6tk8rqW9U/FOv/81AABwWfDrL2Vzc7NKSkq0fv16rVq1ynM+Ly9Pf/zjHz3fjxs3To8//rjuuOMOdXZ2yuG4+K9zOp3KyMjwp5xBJz0pVhlJsapzt+nApy7NuCrV6pIAAIhofg0JLV68WMXFxSosLLzkvS6XS0lJSf2GFUnauXOnRo0apYkTJ2rRokU6derURe9tb2+X2+32Ogabr2QzLAQAwED5HFg2bdqkPXv2qLS09JL3njx5Uo899pgWLlzY731FRUV69tlntWPHDv3iF7/Qrl27NHfuXHV1dfV5f2lpqZKTkz1Hdna2rx/DcvlXpkiSqo43WloHAACDgU9DQjU1NVqyZIm2b9+u2NjYfu91u90qLi7WpEmT9NOf/rTfe2+99VbP15MnT1Z+fr7GjRunnTt3qqCg4IL7V6xYoQcffNDrdw220NI7j6XqWKOldQAAMBj41MNSWVmphoYGTZ06VQ6HQw6HQ7t27dLatWvlcDg8PSJNTU0qKipSYmKiNm/erOjoaJ+Kuuqqq5SWlqZDhw71ed3pdCopKcnrGGzyr0xWlK1nx9sTrjNWlwMAQETzKbAUFBRo//79qqqq8hzTp09XSUmJqqqqZLfb5Xa7NXv2bMXExOill166ZE9MX44fP65Tp04pMzPT558dLBJjo/XlrJ55LO8cOW1xNQAARDafAktiYqLy8vK8jvj4eKWmpiovL88TVlpaWvQf//EfcrvdqqurU11dndd8lNzcXG3evFlSz4qjhx56SG+//baOHj2qHTt2aN68eRo/frzmzJkT3E8bYa7NGSGJwAIAwKUEdQOQPXv2qLy8XJI0fvx4r2tHjhzR2LFjJUnV1dWezeTsdrv27dun//zP/1RjY6OysrI0e/ZsPfbYY0N6LxapJ7D8x5tHCCwAAFxCwIFl586dnq9vuOEGGWMu+TOfvycuLk7btm0LtIxB6atje3pYDjY061Rzu1IThnZAAwDAX7xLyEIj4mN0dXqCJKmcXhYAAC6KwGKxWeNHSpJe/+hvFlcCAEDkIrBY7BsTewLLro/+NqDhNAAALkcEFovNyBkhpyNKJ1xtOtjQbHU5AABEJAKLxWKj7Z6XHzIsBABA3wgsEeAbV/cMC/35g3qLKwEAIDIRWCLA7Enpkno2kDvV3G5xNQAARB4CSwTIHjFMX85KUreRtr9PLwsAAF9EYIkQc/MyJElb/1pncSUAAEQeAkuEKMrredHj/x46qc9azlpcDQAAkYXAEiHGj0rQpMwkdXQZ/WlfrdXlAAAQUQgsEeT/TLtSkvTHyuMWVwIAQGQhsESQeVOy5Iiy6b3jLh2sb7K6HAAAIgaBJYKkJjj1/+SOkiT9/u1PLK4GAIDIQWCJMHddN1aS9HzlcbnOdFhbDAAAEYLAEmFmjktVbkaiWs926b8qjlldDgAAEYHAEmFsNpv+8es5kqT/fOsTdXZ1W1wRAADWI7BEoJunZCk1PkafNp7Ri3s+tbocAAAsR2CJQLHRdt33jXGSpDV//khtHV0WVwQAgLUILBHqOzPHKCMpVrWuNm0sZy4LAODyRmCJULHRdt1fMEGS9NvXDrJdPwDgskZgiWDfmn6lrk5P0GetHfr5Kx9YXQ4AAJYhsESwaHuUSr85WVLPvixvHT5pcUUAAFiDwBLhpo0ZoTu+NlqStOyFfXK1spkcAODyQ2AZBJYV5Wr0iGE6/tkZ/fCF92SMsbokAADCisAyCCTFRuuJ26cqxh6l7e/X68mdh60uCQCAsCKwDBKTr0zWIzdNkiT9clu1Xqg8bnFFAACED4FlELnja2O08PqrJEnL/7hPr+w/YXFFAACEB4FlkPlRUa6+OfUKdXUbfe8Pe3hBIgDgskBgGWSiomz65f/5im67NlvdRlr+x/16/P++z0sSAQBDGoFlELJH2fTzBZP1vb8fL0la/8YR3b6+XEdPtlhcGQAAoUFgGaRsNpt+OGeiflcyVfExdr1z9LSK/vV1/fvrh3W2k94WAMDQQmAZ5OZOztSrS67X18enqq2jWz9/5UMV/MtObd57XF3d7NcCABgabGYI7ELmdruVnJwsl8ulpKQkq8uxhDFGz797XL/8/6r1t6Z2SdIVKXG642tj9A9fzdaI+BiLKwQAwJsvf78JLENM69lObfjfo/p/3/hYn53bxj/abtOs8Wm6cXKmCr+UruGEFwBABCCwQG0dXfrTe7V6dvcn2v+py3PeZpMmZSbpunGpujYnVZOvSFZ6klM2m83CagEAlyMCC7wcamjS/91Xp1cPnNCHdU0XXE9LiNGkrGSNH5mgManDNDp1mMamxuuKlDjFOJjmBAAIDQILLqrB3abdH5/S/x46qfdqXDrY0KT+5uYOHxattASnRib2HGkJTiXFRisx1vG5o+f7eKdDsdF2OR1R5w67ou02em8AAH0isGDA2jq69GFdk/5a69Inp1p19GSLjp1u1dFTLWrrCHx5tM0mT3hxOqIU44iSI8omu+eIkj1KPf+0SY6oKEVF9f7TJkeUTVE2m+xRkk022Ww9z7TJpnP/k81mO/dP7+9l+9zPfK6ezz9Hn7se6lxlU+h+QehrD/HzB3GoHcSlAz5JS3Bq8bn9v4LFl7/fjqD+Zgw6sdF2TclO0ZTsFK/zxhidbjmrk81ndbK5XX9r6jlONrfL3dapprYONbV1qrnd++v2zm6vfWCMkdo6uoMSfgAA1rlqZHzQA4svCCzok81mU2qCU6kJTk1Uok8/291tdLarW+2d3Wrv7FJ7R8/XbR1dau/sVrcx6ur+3GGMurrO/fPcuW5j1PmFc0aSTM8/jekJVZ6v1fO9PN+bz53vqav3nM7d+/nr554eMqHsxwx5F2mIO2FDXX9o237Qd1ADAzZ8mLUrTAksCLqoKJtio+yKjbZLira6HADAEMASEAAAEPEILAAAIOIFFFjKyspks9m0dOlSSdLp06f1/e9/XxMnTlRcXJxGjx6t+++/Xy6Xq9/nGGP0yCOPKDMzU3FxcSosLNTBgwcDKQ0AAAwhfgeWiooKrVu3Tvn5+Z5ztbW1qq2t1a9+9SsdOHBAzzzzjLZu3ap77rmn32etXr1aa9eu1VNPPaXy8nLFx8drzpw5amtr87c8AAAwhPi1D0tzc7OmTp2qJ598UqtWrdKUKVO0Zs2aPu99/vnndccdd6ilpUUOx4VzfI0xysrK0g9+8AP98Ic/lCS5XC6lp6frmWee0a233nrJetiHBQCAwceXv99+9bAsXrxYxcXFKiwsvOS9vUX0FVYk6ciRI6qrq/N6VnJysmbMmKHdu3f3+TPt7e1yu91eBwAAGLp8Xta8adMm7dmzRxUVFZe89+TJk3rssce0cOHCi95TV1cnSUpPT/c6n56e7rn2RaWlpXr00Ud9qBoAAAxmPvWw1NTUaMmSJdq4caNiY2P7vdftdqu4uFiTJk3ST3/600BqvMCKFSvkcrk8R01NTVCfDwAAIotPPSyVlZVqaGjQ1KlTPee6urr0+uuv67e//a3a29tlt9vV1NSkoqIiJSYmavPmzYqOvvjmYRkZGZKk+vp6ZWZmes7X19drypQpff6M0+mU0+n0pXQAADCI+dTDUlBQoP3796uqqspzTJ8+XSUlJaqqqpLdbpfb7dbs2bMVExOjl1566ZI9MTk5OcrIyNCOHTs859xut8rLyzVz5kz/PhUAABhSfOphSUxMVF5ente5+Ph4paamKi8vzxNWWltb9fvf/95rQuzIkSNlt9slSbm5uSotLdWCBQs8+7isWrVKEyZMUE5OjlauXKmsrCzNnz8/OJ8SAAAMakF9l9CePXtUXl4uSRo/3vuNjkeOHNHYsWMlSdXV1V6byS1btkwtLS1auHChGhsbNWvWLG3duvWSvTMAAODy4Nc+LJGGfVgAABh8fPn7PSTe1tybudiPBQCAwaP37/ZA+k6GRGBpamqSJGVnZ1tcCQAA8FVTU5OSk5P7vWdIDAl1d3ertrZWiYmJstlsQX222+1Wdna2ampqGG4KIdo5PGjn8KGtw4N2Do9QtbMxRk1NTcrKylJUVP8Ll4dED0tUVJSuvPLKkP6OpKQk/s8QBrRzeNDO4UNbhwftHB6haOdL9az08vttzQAAAOFCYAEAABGPwHIJTqdT//zP/8yrAEKMdg4P2jl8aOvwoJ3DIxLaeUhMugUAAEMbPSwAACDiEVgAAEDEI7AAAICIR2ABAAARj8DSjyeeeEJjx45VbGysZsyYoXfeecfqkgaV0tJSffWrX1ViYqJGjRql+fPnq7q62uuetrY2LV68WKmpqUpISNAtt9yi+vp6r3uOHTum4uJiDRs2TKNGjdJDDz2kzs7OcH6UQaWsrEw2m01Lly71nKOdg+fTTz/VHXfcodTUVMXFxWny5Ml69913PdeNMXrkkUeUmZmpuLg4FRYW6uDBg17POH36tEpKSpSUlKSUlBTdc889am5uDvdHiVhdXV1auXKlcnJyFBcXp3Hjxumxxx7zet8M7ey7119/XTfddJOysrJks9m0ZcsWr+vBatN9+/bp7/7u7xQbG6vs7GytXr06OB/AoE+bNm0yMTEx5umnnzZ//etfzXe/+12TkpJi6uvrrS5t0JgzZ47ZsGGDOXDggKmqqjI33nijGT16tGlubvbcc99995ns7GyzY8cO8+6775qvfe1r5rrrrvNc7+zsNHl5eaawsNDs3bvXvPLKKyYtLc2sWLHCio8U8d555x0zduxYk5+fb5YsWeI5TzsHx+nTp82YMWPMXXfdZcrLy83HH39stm3bZg4dOuS5p6yszCQnJ5stW7aY9957z9x8880mJyfHnDlzxnNPUVGR+cpXvmLefvtt88Ybb5jx48eb2267zYqPFJEef/xxk5qaal5++WVz5MgR8/zzz5uEhATzr//6r557aGffvfLKK+YnP/mJefHFF40ks3nzZq/rwWhTl8tl0tPTTUlJiTlw4IB57rnnTFxcnFm3bl3A9RNYLuLaa681ixcv9nzf1dVlsrKyTGlpqYVVDW4NDQ1Gktm1a5cxxpjGxkYTHR1tnn/+ec89H3zwgZFkdu/ebYzp+T9YVFSUqaur89zzu9/9ziQlJZn29vbwfoAI19TUZCZMmGC2b99uvvGNb3gCC+0cPMuXLzezZs266PXu7m6TkZFhfvnLX3rONTY2GqfTaZ577jljjDHvv/++kWQqKio897z66qvGZrOZTz/9NHTFDyLFxcXmH//xH73OffOb3zQlJSXGGNo5GL4YWILVpk8++aQZPny41383li9fbiZOnBhwzQwJ9eHs2bOqrKxUYWGh51xUVJQKCwu1e/duCysb3FwulyRpxIgRkqTKykp1dHR4tXNubq5Gjx7taefdu3dr8uTJSk9P99wzZ84cud1u/fWvfw1j9ZFv8eLFKi4u9mpPiXYOppdeeknTp0/Xt771LY0aNUrXXHON1q9f77l+5MgR1dXVebV1cnKyZsyY4dXWKSkpmj59uueewsJCRUVFqby8PHwfJoJdd9112rFjhz766CNJ0nvvvac333xTc+fOlUQ7h0Kw2nT37t26/vrrFRMT47lnzpw5qq6u1meffRZQjUPi5YfBdvLkSXV1dXn9x1uS0tPT9eGHH1pU1eDW3d2tpUuX6utf/7ry8vIkSXV1dYqJiVFKSorXvenp6aqrq/Pc09e/h95r6LFp0ybt2bNHFRUVF1yjnYPn448/1u9+9zs9+OCD+vGPf6yKigrdf//9iomJ0Z133ulpq77a8vNtPWrUKK/rDodDI0aMoK3P+dGPfiS3263c3FzZ7XZ1dXXp8ccfV0lJiSTRziEQrDatq6tTTk7OBc/ovTZ8+HC/aySwICwWL16sAwcO6M0337S6lCGnpqZGS5Ys0fbt2xUbG2t1OUNad3e3pk+frp///OeSpGuuuUYHDhzQU089pTvvvNPi6oaO//7v/9bGjRv1hz/8QV/+8pdVVVWlpUuXKisri3a+jDEk1Ie0tDTZ7fYLVlHU19crIyPDoqoGr+9973t6+eWX9Ze//EVXXnml53xGRobOnj2rxsZGr/s/384ZGRl9/nvovYaeIZ+GhgZNnTpVDodDDodDu3bt0tq1a+VwOJSenk47B0lmZqYmTZrkde5LX/qSjh07Jul8W/X3346MjAw1NDR4Xe/s7NTp06dp63Meeugh/ehHP9Ktt96qyZMn6zvf+Y4eeOABlZaWSqKdQyFYbRrK/5YQWPoQExOjadOmaceOHZ5z3d3d2rFjh2bOnGlhZYOLMUbf+973tHnzZr322msXdBNOmzZN0dHRXu1cXV2tY8eOedp55syZ2r9/v9f/SbZv366kpKQL/nBcrgoKCrR//35VVVV5junTp6ukpMTzNe0cHF//+tcvWJr/0UcfacyYMZKknJwcZWRkeLW12+1WeXm5V1s3NjaqsrLSc89rr72m7u5uzZgxIwyfIvK1trYqKsr7z5Pdbld3d7ck2jkUgtWmM2fO1Ouvv66Ojg7PPdu3b9fEiRMDGg6SxLLmi9m0aZNxOp3mmWeeMe+//75ZuHChSUlJ8VpFgf4tWrTIJCcnm507d5oTJ054jtbWVs899913nxk9erR57bXXzLvvvmtmzpxpZs6c6bneu9x29uzZpqqqymzdutWMHDmS5baX8PlVQsbQzsHyzjvvGIfDYR5//HFz8OBBs3HjRjNs2DDz+9//3nNPWVmZSUlJMf/zP/9j9u3bZ+bNm9fn0tBrrrnGlJeXmzfffNNMmDDhsl5u+0V33nmnueKKKzzLml988UWTlpZmli1b5rmHdvZdU1OT2bt3r9m7d6+RZP7lX/7F7N2713zyySfGmOC0aWNjo0lPTzff+c53zIEDB8ymTZvMsGHDWNYcav/2b/9mRo8ebWJiYsy1115r3n77batLGlQk9Xls2LDBc8+ZM2fMP/3TP5nhw4ebYcOGmQULFpgTJ054Pefo0aNm7ty5Ji4uzqSlpZkf/OAHpqOjI8yfZnD5YmChnYPnT3/6k8nLyzNOp9Pk5uaaf//3f/e63t3dbVauXGnS09ON0+k0BQUFprq62uueU6dOmdtuu80kJCSYpKQkc/fdd5umpqZwfoyI5na7zZIlS8zo0aNNbGysueqqq8xPfvITr6WytLPv/vKXv/T53+Q777zTGBO8Nn3vvffMrFmzjNPpNFdccYUpKysLSv02Yz63dSAAAEAEYg4LAACIeAQWAAAQ8QgsAAAg4hFYAABAxCOwAACAiEdgAQAAEY/AAgAAIh6BBQAARDwCCwAAiHgEFgAAEPEILAAAIOIRWAAAQMT7/wHkPjIwi4VeygAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = FCDPCN(\n",
    "    # sizes=[INPUT_SHAPE, 1024, 512, 512],\n",
    "    # precisions=[-9999, 4.0, 2.0, 7.0],\n",
    "    sizes=[INPUT_SHAPE, 128],\n",
    "    precisions=[1.0, 1.0],\n",
    "    bias=True, \n",
    "    symmetric=True,\n",
    "    actv_fn=F.relu,\n",
    "    steps=250,\n",
    "    gamma=0.001,\n",
    "    x_decay=0.0,\n",
    "    dropout=0.0,\n",
    "    has_top=False,\n",
    ").to(device)\n",
    "\n",
    "x = train_dataset.transformed_images.flatten(1)\n",
    "gamma = torch.ones(x.shape[0], device=device) * model.gamma\n",
    "state, x_optimiser = model.init_state(x, pin_obs=True)\n",
    "\n",
    "vfes = []\n",
    "for _ in range(1000):\n",
    "    model.step(state, x_optimiser, gamma)\n",
    "    with torch.no_grad():\n",
    "        vfes.append(model.vfe(state).item())\n",
    "\n",
    "print(f'Initial VFE: {vfes[0]:.4f}, Final VFE: {vfes[-1]:.4f}')\n",
    "plt.plot(vfes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                 \r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 8\u001b[0m\n\u001b[0;32m      6\u001b[0m log_dir\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m      7\u001b[0m model_dir \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m \u001b[43mtrain_iPC\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43msupervised\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43mval_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mNUM_EPOCHS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43meval_every\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m250\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.001\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     16\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mBATCH_SIZE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     17\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreg_coeff\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.02\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     18\u001b[0m \u001b[43m    \u001b[49m\u001b[43mflatten\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     19\u001b[0m \u001b[43m    \u001b[49m\u001b[43mneg_coeff\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     20\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcd_coeff\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     21\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcd_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     22\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlog_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlog_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     23\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     24\u001b[0m \u001b[43m    \u001b[49m\u001b[43mminimal_stats\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     25\u001b[0m \u001b[43m    \u001b[49m\u001b[43massert_grads\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     26\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mAdamW\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     27\u001b[0m \u001b[43m    \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     28\u001b[0m \u001b[43m    \u001b[49m\u001b[43mno_momentum\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     29\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnorm_grads\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     30\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnorm_weights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     31\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlearn_layer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     32\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m     33\u001b[0m \u001b[38;5;66;03m# train_iPC_classifier(\u001b[39;00m\n\u001b[0;32m     34\u001b[0m \u001b[38;5;66;03m#     model,\u001b[39;00m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;66;03m#     classifier,\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     45\u001b[0m \u001b[38;5;66;03m#     scheduler=None,\u001b[39;00m\n\u001b[0;32m     46\u001b[0m \u001b[38;5;66;03m# )\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\joeag\\Documents\\pclib\\pclib\\optim\\train_DeepiPC.py:391\u001b[0m, in \u001b[0;36mtrain_iPC\u001b[1;34m(model, supervised, train_data, val_data, num_epochs, eval_every, lr, batch_size, reg_coeff, flatten, neg_coeff, cd_coeff, cd_steps, log_dir, model_dir, minimal_stats, assert_grads, optim, scheduler, no_momentum, norm_grads, norm_weights, learn_layer)\u001b[0m\n\u001b[0;32m    389\u001b[0m \u001b[38;5;66;03m# Collects statistics for validation data if it exists\u001b[39;00m\n\u001b[0;32m    390\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m val_data \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m epoch \u001b[38;5;241m%\u001b[39m eval_every \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m--> 391\u001b[0m     val_results \u001b[38;5;241m=\u001b[39m \u001b[43mval_pass\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mflatten\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlearn_layer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlearn_layer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    392\u001b[0m     stats[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_vfe\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m val_results[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvfe\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mitem()\n\u001b[0;32m    394\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m log_dir:\n",
      "File \u001b[1;32mc:\\Users\\joeag\\Documents\\pclib\\pclib\\optim\\train_DeepiPC.py:163\u001b[0m, in \u001b[0;36mval_pass\u001b[1;34m(model, val_loader, flatten, learn_layer)\u001b[0m\n\u001b[0;32m    160\u001b[0m         x \u001b[38;5;241m=\u001b[39m images\n\u001b[0;32m    162\u001b[0m     \u001b[38;5;66;03m# Forward pass\u001b[39;00m\n\u001b[1;32m--> 163\u001b[0m     _, state, _ \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpin_obs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlearn_layer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlearn_layer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    164\u001b[0m     vfe \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mvfe(state, batch_reduction\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msum\u001b[39m\u001b[38;5;124m'\u001b[39m, learn_layer\u001b[38;5;241m=\u001b[39mlearn_layer)\n\u001b[0;32m    166\u001b[0m vfe \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(val_loader\u001b[38;5;241m.\u001b[39mdataset)\n",
      "File \u001b[1;32mc:\\Users\\joeag\\Documents\\venvs\\ml-env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\joeag\\Documents\\pclib\\pclib\\nn\\models\\fcdpcn.py:273\u001b[0m, in \u001b[0;36mFCDPCN.forward\u001b[1;34m(self, obs, y, pin_obs, pin_target, steps, learn_layer)\u001b[0m\n\u001b[0;32m    271\u001b[0m gamma \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mones(state[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mx\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m])\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice) \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgamma\n\u001b[0;32m    272\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(steps):\n\u001b[1;32m--> 273\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimiser\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgamma\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    274\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m    275\u001b[0m         gamma, prev_vfe \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mupdate_gamma(state, gamma, prev_vfe)\n",
      "File \u001b[1;32mc:\\Users\\joeag\\Documents\\pclib\\pclib\\nn\\models\\fcdpcn.py:231\u001b[0m, in \u001b[0;36mFCDPCN.step\u001b[1;34m(self, state, optimiser, gamma)\u001b[0m\n\u001b[0;32m    229\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m p\u001b[38;5;241m.\u001b[39mgrad \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGradients should be zeroed before calling step().\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    230\u001b[0m vfe \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvfe(state, batch_reduction\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnone\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;241m*\u001b[39m gamma)\u001b[38;5;241m.\u001b[39msum() \n\u001b[1;32m--> 231\u001b[0m \u001b[43mmake_dot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvfe\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrender\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdeep_vfe\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpng\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    232\u001b[0m vfe\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m    233\u001b[0m \u001b[38;5;66;03m# for p in optimiser.param_groups[0]['params']:\u001b[39;00m\n\u001b[0;32m    234\u001b[0m \u001b[38;5;66;03m#     print(f'grad.mean(): {p.grad.mean()}, grad.std(): {p.grad.std()}')\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\joeag\\Documents\\venvs\\ml-env\\Lib\\site-packages\\graphviz\\_tools.py:171\u001b[0m, in \u001b[0;36mdeprecate_positional_args.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    162\u001b[0m     wanted \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvalue\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    163\u001b[0m                        \u001b[38;5;28;01mfor\u001b[39;00m name, value \u001b[38;5;129;01min\u001b[39;00m deprecated\u001b[38;5;241m.\u001b[39mitems())\n\u001b[0;32m    164\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mThe signature of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m will be reduced\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    165\u001b[0m                   \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msupported_number\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m positional args\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    166\u001b[0m                   \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlist\u001b[39m(supported)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: pass \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mwanted\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    167\u001b[0m                   \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m as keyword arg(s)\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m    168\u001b[0m                   stacklevel\u001b[38;5;241m=\u001b[39mstacklevel,\n\u001b[0;32m    169\u001b[0m                   category\u001b[38;5;241m=\u001b[39mcategory)\n\u001b[1;32m--> 171\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\joeag\\Documents\\venvs\\ml-env\\Lib\\site-packages\\graphviz\\rendering.py:122\u001b[0m, in \u001b[0;36mRender.render\u001b[1;34m(self, filename, directory, view, cleanup, format, renderer, formatter, neato_no_op, quiet, quiet_view, outfile, engine, raise_if_result_exists, overwrite_source)\u001b[0m\n\u001b[0;32m    118\u001b[0m filepath \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msave(filename, directory\u001b[38;5;241m=\u001b[39mdirectory, skip_existing\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m    120\u001b[0m args\u001b[38;5;241m.\u001b[39mappend(filepath)\n\u001b[1;32m--> 122\u001b[0m rendered \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_render\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    124\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cleanup:\n\u001b[0;32m    125\u001b[0m     log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdelete \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m'\u001b[39m, filepath)\n",
      "File \u001b[1;32mc:\\Users\\joeag\\Documents\\venvs\\ml-env\\Lib\\site-packages\\graphviz\\_tools.py:171\u001b[0m, in \u001b[0;36mdeprecate_positional_args.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    162\u001b[0m     wanted \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvalue\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    163\u001b[0m                        \u001b[38;5;28;01mfor\u001b[39;00m name, value \u001b[38;5;129;01min\u001b[39;00m deprecated\u001b[38;5;241m.\u001b[39mitems())\n\u001b[0;32m    164\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mThe signature of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m will be reduced\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    165\u001b[0m                   \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msupported_number\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m positional args\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    166\u001b[0m                   \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlist\u001b[39m(supported)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: pass \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mwanted\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    167\u001b[0m                   \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m as keyword arg(s)\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m    168\u001b[0m                   stacklevel\u001b[38;5;241m=\u001b[39mstacklevel,\n\u001b[0;32m    169\u001b[0m                   category\u001b[38;5;241m=\u001b[39mcategory)\n\u001b[1;32m--> 171\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\joeag\\Documents\\venvs\\ml-env\\Lib\\site-packages\\graphviz\\backend\\rendering.py:324\u001b[0m, in \u001b[0;36mrender\u001b[1;34m(engine, format, filepath, renderer, formatter, neato_no_op, quiet, outfile, raise_if_result_exists, overwrite_filepath)\u001b[0m\n\u001b[0;32m    320\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mFileExistsError(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124moutput file exists: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mos\u001b[38;5;241m.\u001b[39mfspath(outfile)\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    322\u001b[0m cmd \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m args\n\u001b[1;32m--> 324\u001b[0m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_check\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcmd\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    325\u001b[0m \u001b[43m                  \u001b[49m\u001b[43mcwd\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilepath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparent\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mfilepath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparts\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    326\u001b[0m \u001b[43m                  \u001b[49m\u001b[43mquiet\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquiet\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    327\u001b[0m \u001b[43m                  \u001b[49m\u001b[43mcapture_output\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    329\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m os\u001b[38;5;241m.\u001b[39mfspath(outfile)\n",
      "File \u001b[1;32mc:\\Users\\joeag\\Documents\\venvs\\ml-env\\Lib\\site-packages\\graphviz\\backend\\execute.py:81\u001b[0m, in \u001b[0;36mrun_check\u001b[1;34m(cmd, input_lines, encoding, quiet, **kwargs)\u001b[0m\n\u001b[0;32m     79\u001b[0m         proc \u001b[38;5;241m=\u001b[39m _run_input_lines(cmd, input_lines, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[0;32m     80\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 81\u001b[0m         proc \u001b[38;5;241m=\u001b[39m \u001b[43msubprocess\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcmd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     82\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     83\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m e\u001b[38;5;241m.\u001b[39merrno \u001b[38;5;241m==\u001b[39m errno\u001b[38;5;241m.\u001b[39mENOENT:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\subprocess.py:548\u001b[0m, in \u001b[0;36mrun\u001b[1;34m(input, capture_output, timeout, check, *popenargs, **kwargs)\u001b[0m\n\u001b[0;32m    545\u001b[0m     kwargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstdout\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m PIPE\n\u001b[0;32m    546\u001b[0m     kwargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstderr\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m PIPE\n\u001b[1;32m--> 548\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mPopen\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mpopenargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m process:\n\u001b[0;32m    549\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    550\u001b[0m         stdout, stderr \u001b[38;5;241m=\u001b[39m process\u001b[38;5;241m.\u001b[39mcommunicate(\u001b[38;5;28minput\u001b[39m, timeout\u001b[38;5;241m=\u001b[39mtimeout)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\subprocess.py:1026\u001b[0m, in \u001b[0;36mPopen.__init__\u001b[1;34m(self, args, bufsize, executable, stdin, stdout, stderr, preexec_fn, close_fds, shell, cwd, env, universal_newlines, startupinfo, creationflags, restore_signals, start_new_session, pass_fds, user, group, extra_groups, encoding, errors, text, umask, pipesize, process_group)\u001b[0m\n\u001b[0;32m   1022\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtext_mode:\n\u001b[0;32m   1023\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstderr \u001b[38;5;241m=\u001b[39m io\u001b[38;5;241m.\u001b[39mTextIOWrapper(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstderr,\n\u001b[0;32m   1024\u001b[0m                     encoding\u001b[38;5;241m=\u001b[39mencoding, errors\u001b[38;5;241m=\u001b[39merrors)\n\u001b[1;32m-> 1026\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execute_child\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexecutable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreexec_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclose_fds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1027\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mpass_fds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcwd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1028\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mstartupinfo\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreationflags\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshell\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1029\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mp2cread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mp2cwrite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1030\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mc2pread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mc2pwrite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1031\u001b[0m \u001b[43m                        \u001b[49m\u001b[43merrread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrwrite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1032\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mrestore_signals\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1033\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mgid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mumask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1034\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mstart_new_session\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprocess_group\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1035\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[0;32m   1036\u001b[0m     \u001b[38;5;66;03m# Cleanup if the child failed starting.\u001b[39;00m\n\u001b[0;32m   1037\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mfilter\u001b[39m(\u001b[38;5;28;01mNone\u001b[39;00m, (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstdin, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstdout, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstderr)):\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\subprocess.py:1538\u001b[0m, in \u001b[0;36mPopen._execute_child\u001b[1;34m(self, args, executable, preexec_fn, close_fds, pass_fds, cwd, env, startupinfo, creationflags, shell, p2cread, p2cwrite, c2pread, c2pwrite, errread, errwrite, unused_restore_signals, unused_gid, unused_gids, unused_uid, unused_umask, unused_start_new_session, unused_process_group)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# Start the process\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1538\u001b[0m     hp, ht, pid, tid \u001b[38;5;241m=\u001b[39m _winapi\u001b[38;5;241m.\u001b[39mCreateProcess(executable, args,\n\u001b[0;32m   1539\u001b[0m                              \u001b[38;5;66;03m# no special security\u001b[39;00m\n\u001b[0;32m   1540\u001b[0m                              \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   1541\u001b[0m                              \u001b[38;5;28mint\u001b[39m(\u001b[38;5;129;01mnot\u001b[39;00m close_fds),\n\u001b[0;32m   1542\u001b[0m                              creationflags,\n\u001b[0;32m   1543\u001b[0m                              env,\n\u001b[0;32m   1544\u001b[0m                              cwd,\n\u001b[0;32m   1545\u001b[0m                              startupinfo)\n\u001b[0;32m   1546\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m   1547\u001b[0m     \u001b[38;5;66;03m# Child is launched. Close the parent's copy of those pipe\u001b[39;00m\n\u001b[0;32m   1548\u001b[0m     \u001b[38;5;66;03m# handles that only the child should have open.  You need\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;66;03m# pipe will not close when the child process exits and the\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m     \u001b[38;5;66;03m# ReadFile will hang.\u001b[39;00m\n\u001b[0;32m   1553\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_close_pipe_fds(p2cread, p2cwrite,\n\u001b[0;32m   1554\u001b[0m                          c2pread, c2pwrite,\n\u001b[0;32m   1555\u001b[0m                          errread, errwrite)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# torch.manual_seed(42)\n",
    "model_name = \"128\"\n",
    "NUM_EPOCHS = 2500\n",
    "BATCH_SIZE = 48000\n",
    "# log_dir = f'examples/mnist/out/logs/fc/unsup_iPC/x_decay/{model_name}'\n",
    "log_dir=None\n",
    "model_dir = None\n",
    "train_iPC(\n",
    "    model=model,\n",
    "    supervised=False,\n",
    "    train_data=train_dataset,\n",
    "    val_data=val_dataset,\n",
    "    num_epochs=NUM_EPOCHS,\n",
    "    eval_every=250,\n",
    "    lr=0.001,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    reg_coeff=0.02,\n",
    "    flatten=True,\n",
    "    neg_coeff=None,\n",
    "    cd_coeff=None,\n",
    "    cd_steps=1,\n",
    "    log_dir=log_dir,\n",
    "    model_dir=model_dir,\n",
    "    minimal_stats=False,\n",
    "    assert_grads=False,\n",
    "    optim='AdamW',\n",
    "    scheduler=None,\n",
    "    no_momentum=False,\n",
    "    norm_grads=False,\n",
    "    norm_weights=False,\n",
    "    learn_layer=None,\n",
    ")\n",
    "# train_iPC_classifier(\n",
    "#     model,\n",
    "#     classifier,\n",
    "#     train_dataset,\n",
    "#     val_dataset,\n",
    "#     num_epochs=100,\n",
    "#     lr=0.001,\n",
    "#     batch_size=100,\n",
    "#     reg_coeff=0.02,\n",
    "#     flatten=True,\n",
    "#     log_dir=log_dir,\n",
    "#     optim='AdamW',\n",
    "#     scheduler=None,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 32, 28, 28])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnn = nn.Sequential(\n",
    "    nn.Unflatten(1, (128, 1, 1)),\n",
    "    nn.ConvTranspose2d(128, 32, 7, stride=1),\n",
    "    # nn.ConvTranspose2d(64, 32, 5, stride=2),\n",
    "    nn.ConvTranspose2d(32, 32, 4, stride=4),\n",
    "    # nn.ConvTranspose2d(32, 1, 4, stride=1),\n",
    ").to(device)\n",
    "\n",
    "z = torch.randn(10, 128).to(device)\n",
    "cnn(z).shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "41.36286163330078"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = val_dataset.transformed_images[:100].flatten(1)\n",
    "\n",
    "_, state, _ = model(x, pin_obs=True)\n",
    "\n",
    "# state, optimiser = model.init_state(x, pin_obs=True)\n",
    "# gamma = torch.ones(x.shape[0], device=device) * model.gamma\n",
    "# for _ in range(250):\n",
    "#     model.step(state, optimiser, gamma)\n",
    "\n",
    "\n",
    "\n",
    "vfe = model.vfe(state).item()\n",
    "vfe\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp = torch.nn.Linear(10, 2)\n",
    "\n",
    "state = {\"x\": torch.ones((1, 10), requires_grad=True)}\n",
    "optimiser = torch.optim.SGD([state[\"x\"]], lr=0.001)\n",
    "x = torch.randn((1,2))\n",
    "\n",
    "for _ in range(5):\n",
    "    state_flatt = state[\"x\"].flatten(1)\n",
    "    pred = mlp(state_flatt)\n",
    "    error = (x - pred).square().sum()\n",
    "    optimiser.zero_grad()\n",
    "    error.backward()\n",
    "    optimiser.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(1, 784).to(device)\n",
    "x.requires_grad = True\n",
    "y = F.relu(x)\n",
    "y.requires_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(1,1)\n",
    "from torchviz import make_dot\n",
    "make_dot(x).render('test', format=\"png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decays = [0.01, 0.05, 0.1, 0.5, 1.0]\n",
    "for decay in decays:\n",
    "    torch.manual_seed(seed)\n",
    "\n",
    "    model_name = f'{decay}'\n",
    "    model = FCPCN(\n",
    "        sizes=[INPUT_SHAPE, 128],\n",
    "        # precisions=[-9999, 4.0, 2.0, 7.0],\n",
    "        precisions=[1.0, 1.0],\n",
    "        bias=True, \n",
    "        symmetric=True,\n",
    "        actv_fn=F.relu,\n",
    "        steps=250,\n",
    "        gamma=0.001,\n",
    "        x_decay=decay,\n",
    "        dropout=0.0,\n",
    "        has_top=False,\n",
    "    ).to(device)\n",
    "    classifier = Classifier(128, 10).to(device)\n",
    "    stats = {\n",
    "        'epoch': 0,\n",
    "        'posVfe': [],\n",
    "        'negVfe': [],\n",
    "        'negMse': [],\n",
    "        'cdVfe': [],\n",
    "        'valVfe': [],\n",
    "        'valAcc': [],\n",
    "    }\n",
    "\n",
    "    NUM_EPOCHS = 2500\n",
    "    BATCH_SIZE = 48000\n",
    "    log_dir = f'examples/mnist/out/logs/fc/unsup_iPC/x_decay/{model_name}'\n",
    "    # model_dir = f'examples/mnist/out/models/fc/unsup_iPC/layers/{model_name}'\n",
    "    # log_dir = None\n",
    "    model_dir = None\n",
    "    train_iPC(\n",
    "        model=model,\n",
    "        supervised=False,\n",
    "        train_data=train_dataset,\n",
    "        val_data=val_dataset,\n",
    "        num_epochs=NUM_EPOCHS,\n",
    "        eval_every=250,\n",
    "        lr=0.001,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        reg_coeff=0.02,\n",
    "        flatten=True,\n",
    "        neg_coeff=None,\n",
    "        cd_coeff=None,\n",
    "        cd_steps=1,\n",
    "        log_dir=log_dir,\n",
    "        model_dir=model_dir,\n",
    "        minimal_stats=False,\n",
    "        assert_grads=False,\n",
    "        optim='AdamW',\n",
    "        scheduler=None,\n",
    "        no_momentum=False,\n",
    "        norm_grads=False,\n",
    "        norm_weights=False,\n",
    "        learn_layer=None,\n",
    "    )\n",
    "\n",
    "    train_iPC_classifier(\n",
    "        model,\n",
    "        classifier,\n",
    "        train_dataset,\n",
    "        val_dataset,\n",
    "        num_epochs=100,\n",
    "        lr=0.001,\n",
    "        batch_size=100,\n",
    "        reg_coeff=0.02,\n",
    "        flatten=True,\n",
    "        log_dir=log_dir,\n",
    "        optim='AdamW',\n",
    "        scheduler=None,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sizes_arr = [INPUT_SHAPE, 128], [INPUT_SHAPE, 128, 128], [INPUT_SHAPE, 128, 128, 128], [INPUT_SHAPE, 128, 128, 128, 128]]\n",
    "for sizes in sizes_arr:\n",
    "    torch.manual_seed(seed)\n",
    "\n",
    "    model_name = f'{sizes}'\n",
    "    model = FCPCN(\n",
    "        # sizes=[INPUT_SHAPE, 1024, 512, 512],\n",
    "        # precisions=[-9999, 4.0, 2.0, 7.0],\n",
    "        sizes=sizes,\n",
    "        precisions=[1.0 for _ in range(len(sizes))],\n",
    "        bias=True, \n",
    "        symmetric=True,\n",
    "        actv_fn=F.elu,\n",
    "        steps=250,\n",
    "        gamma=0.001,\n",
    "        x_decay=0.0,\n",
    "        dropout=0.0,\n",
    "        has_top=False,\n",
    "    ).to(device)\n",
    "    # classifier = Classifier(model.sizes[-1], 10).to(device)\n",
    "    \n",
    "    stats = {\n",
    "        'epoch': 0,\n",
    "        'posVfe': [],\n",
    "        'negVfe': [],\n",
    "        'negMse': [],\n",
    "        'cdVfe': [],\n",
    "        'valVfe': [],\n",
    "        'valAcc': [],\n",
    "    }\n",
    "\n",
    "    NUM_EPOCHS = 2500\n",
    "    BATCH_SIZE = 48000\n",
    "    log_dir = f'examples/mnist/out/logs/fc/unsup_iPC/layers/{model_name}'\n",
    "    # model_dir = f'examples/mnist/out/models/fc/unsup_iPC/layers/{model_name}'\n",
    "    # log_dir = None\n",
    "    model_dir = None\n",
    "    train_iPC(\n",
    "        model=model,\n",
    "        supervised=False,\n",
    "        train_data=train_dataset,\n",
    "        val_data=val_dataset,\n",
    "        num_epochs=NUM_EPOCHS,\n",
    "        eval_every=250,\n",
    "        lr=0.001,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        reg_coeff=0.02,\n",
    "        flatten=True,\n",
    "        neg_coeff=None,\n",
    "        cd_coeff=None,\n",
    "        cd_steps=1,\n",
    "        log_dir=log_dir,\n",
    "        model_dir=model_dir,\n",
    "        minimal_stats=False,\n",
    "        assert_grads=False,\n",
    "        optim='AdamW',\n",
    "        scheduler=None,\n",
    "        no_momentum=False,\n",
    "        norm_grads=False,\n",
    "        norm_weights=False,\n",
    "        learn_layer=None,\n",
    "    )\n",
    "\n",
    "    train_iPC_classifier(\n",
    "        model,\n",
    "        classifier,\n",
    "        train_dataset,\n",
    "        val_dataset,\n",
    "        num_epochs=100,\n",
    "        lr=0.001,\n",
    "        batch_size=100,\n",
    "        reg_coeff=0.02,\n",
    "        flatten=True,\n",
    "        log_dir=log_dir,\n",
    "        optim='AdamW',\n",
    "        scheduler=None,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(100, 784).to(device)\n",
    "out, _ = model(x)\n",
    "out.norm()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "states = [torch.ones(1, requires_grad=True), torch.ones(1, requires_grad=False)]\n",
    "substates = [s for s in states if s.requires_grad]\n",
    "len(substates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "imgs = train_dataset.images.clone().flatten(1)\n",
    "imgs.shape\n",
    "out, state = model(imgs, pin_obs=True, steps=200)\n",
    "print(model.vfe(state))\n",
    "\n",
    "sparsity = [(torch.abs(layer['x']) == 0).float().mean().item() for layer in state]\n",
    "print(f'Sparsities: {sparsity}')\n",
    "\n",
    "reconstructions = model.layers[1].predict(state[1])\n",
    "mse = F.mse_loss(reconstructions, imgs)\n",
    "print(mse)\n",
    "\n",
    "# show first 10 images\n",
    "fig, axes = plt.subplots(1, 10, figsize=(15, 5))\n",
    "for i, ax in enumerate(axes):\n",
    "    ax.imshow(reconstructions[i].view(28, 28).detach().cpu(), cmap='gray')\n",
    "    ax.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "imgs = train_dataset.transformed_images.clone().flatten(1) + 0.3*torch.randn_like(train_dataset.transformed_images.clone().flatten(1))\n",
    "state = model.init_state(imgs)\n",
    "\n",
    "vfes = []\n",
    "gamma = torch.ones(imgs.shape[0], device=device) * model.gamma\n",
    "for _ in range(500):\n",
    "    model.step(state, gamma=gamma, pin_obs=True)\n",
    "    vfes.append(model.vfe(state).item())\n",
    "\n",
    "plt.plot(vfes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "imgs = train_dataset.transformed_images[:10].clone().flatten(1)\n",
    "imgs.shape\n",
    "out, state = model(imgs, pin_obs=True, steps=2000)\n",
    "\n",
    "out, state = model(y=out, pin_target=True, steps=500)\n",
    "# out, state = model(imgs, y=out, steps=200)\n",
    "reconstructions = state[0]['x']\n",
    "mse = F.mse_loss(reconstructions, imgs)\n",
    "print(mse)\n",
    "\n",
    "# show first 10 images\n",
    "fig, axes = plt.subplots(1, 10, figsize=(15, 5))\n",
    "for i, ax in enumerate(axes):\n",
    "    ax.imshow(state[0]['x'][i].view(28, 28).detach().cpu(), cmap='gray')\n",
    "    ax.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs = train_dataset.transformed_images.clone().flatten(1)\n",
    "imgs.shape\n",
    "out, state = model(imgs, pin_obs=True, steps=2000)\n",
    "print('vfe: ', model.vfe(state).item())\n",
    "\n",
    "mse = []\n",
    "state = model.init_state(y=out)\n",
    "gamma = torch.ones(state[0]['x'].shape[0], device=device) * model.gamma\n",
    "prev_vfe = None\n",
    "with torch.no_grad():\n",
    "    for _ in range(2000):\n",
    "        model.step(state, gamma, pin_target=True)\n",
    "        reconstructions = state[0]['x']\n",
    "        mse.append(F.mse_loss(reconstructions, imgs).item())\n",
    "        gamma, prev_vfe = model.update_gamma(state, gamma, prev_vfe)\n",
    "\n",
    "print('final mse: ', mse[-1])\n",
    "plt.plot(mse)\n",
    "plt.yscale('log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(stats['negMse'])\n",
    "plt.yscale('log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_stats['classic'] = stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k, v in all_stats.items():\n",
    "    plt.plot(v['posVfe'], label=k)\n",
    "    plt.plot(v['negVfe'], label=k+' (neg)')\n",
    "plt.legend()\n",
    "plt.yscale('log')\n",
    "plt.title('TrainVFE')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "og = train_dataset.images[0].flatten(1).clone()\n",
    "x = og.clone() + torch.randn_like(og) * 0.3\n",
    "# x[torch.rand_like(x) > 0.7] = 0.0\n",
    "# errs = [F.mse_loss(x, og).item()]\n",
    "vfes = []\n",
    "errs = []\n",
    "\n",
    "\n",
    "# Plot the first 1st digit after multiple steps\n",
    "fig, axes = plt.subplots(10, 10, figsize=(10, 10))\n",
    "for i, ax in enumerate(axes):\n",
    "    for j, ax2 in enumerate(ax):\n",
    "        img = x.cpu().reshape(28,28).detach()\n",
    "        # img = (F.normalize(x.cpu(), 1).reshape(28,28).detach() - F.normalize(og.cpu(), 1).reshape(28,28).detach()).abs()\n",
    "        ax2.imshow(img, cmap='gray')\n",
    "        x, state = model.reconstruct(x, steps=200, beta=0.05)\n",
    "        errs.append(F.mse_loss(F.normalize(x), F.normalize(og)).item())\n",
    "        # errs.append(F.mse_loss(x, og).item())\n",
    "        vfes.append(model.vfe(state).item())\n",
    "        ax2.axis('off')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "print('Final MSE: ', errs[-1])\n",
    "print('Final VFE: ', vfes[-1])\n",
    "print('Min MSE: ', min(errs))\n",
    "print('Min VFE: ', min(vfes))\n",
    "# Plot MSE and VFE, VFE logarithmically\n",
    "fig, axes = plt.subplots(1, 2, figsize=(10, 5))\n",
    "axes[0].plot(errs)\n",
    "axes[0].set_title('MSE')\n",
    "axes[1].plot(vfes)\n",
    "axes[1].set_yscale('log')\n",
    "axes[1].set_title('VFE')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'examples\\mnist\\out\\models\\mnist_original.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_classifier(\n",
    "        model,\n",
    "        classifier,\n",
    "        NUM_EPOCHS,\n",
    "        train_dataset,\n",
    "        val_dataset,\n",
    "        stats,\n",
    "):\n",
    "    # Init data\n",
    "    trainLoader = torch.utils.data.DataLoader(train_dataset, batch_size=200, shuffle=True)\n",
    "    valLoader = torch.utils.data.DataLoader(val_dataset, batch_size=200, shuffle=False)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        train_X = [(model(x.flatten(1), pin_obs=True)[0], y) for x, y in trainLoader]\n",
    "\n",
    "    for i in range(NUM_EPOCHS):\n",
    "        classifier.train()\n",
    "        # Epoch stats\n",
    "        epochLosses = []\n",
    "\n",
    "        optimiser = torch.optim.AdamW(classifier.parameters(), lr=0.01)  \n",
    "\n",
    "        # TQDM loop\n",
    "        loop = tqdm(trainLoader, total=len(trainLoader), leave=False)\n",
    "        loop.set_description('Epoch [{}/{}]'.format(i, NUM_EPOCHS))\n",
    "        if stats['epoch'] > 0:\n",
    "            loop.set_postfix({'trainLoss': stats['trainLoss'][-1], 'valLoss': stats['valLoss'][-1], 'valAcc': stats['valAcc'][-1]})\n",
    "\n",
    "        # Iterate over all batches\n",
    "        # for (images, labels) in loop:\n",
    "        #     images = images.flatten(1)\n",
    "\n",
    "        #     with torch.no_grad():\n",
    "        #         # out, state = model(images, pin_obs=True, steps=200)\n",
    "        #         out, state = model(images, pin_obs=True)\n",
    "        for i in range(len(train_X)):\n",
    "            out = train_X[i][0]\n",
    "            labels = train_X[i][1]\n",
    "            out = classifier(out)\n",
    "\n",
    "            model.zero_grad()\n",
    "            loss = F.cross_entropy(out, labels)\n",
    "            loss.backward()\n",
    "            optimiser.step()\n",
    "            epochLosses.append(loss.item())\n",
    "\n",
    "        # Track epoch stats\n",
    "        stats['trainLoss'].append(torch.tensor(epochLosses).mean().item())\n",
    "\n",
    "        # Update learning rate\n",
    "        # if scheduler is not None:\n",
    "        #     scheduler.step(stats['ValAcc'][-1])\n",
    "\n",
    "        # Validation pass\n",
    "        classifier.eval()\n",
    "        with torch.no_grad():\n",
    "            num_correct = 0\n",
    "            N = 0\n",
    "            losses = []\n",
    "            for (images, labels) in valLoader:\n",
    "                images = images.flatten(1)\n",
    "                out, state = model(images, pin_obs=True)\n",
    "                out = classifier(out)\n",
    "                num_correct += (out.argmax(1) == labels).sum().item()\n",
    "                N += len(labels)\n",
    "                losses.append(F.cross_entropy(out, labels).item())\n",
    "            val_results = {\n",
    "                'acc': num_correct / N,\n",
    "                'loss': torch.tensor(losses).mean().item()\n",
    "            }\n",
    "            stats['valAcc'].append(val_results['acc'])\n",
    "            stats['valLoss'].append(val_results['loss'])\n",
    "\n",
    "        stats['epoch'] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = torch.nn.Sequential(\n",
    "    # torch.nn.Linear(600, 200),\n",
    "    # torch.nn.ReLU(),\n",
    "    # torch.nn.Linear(200, 10, bias=False),\n",
    "    torch.nn.Linear(64, 10, bias=False),\n",
    ").to(device)\n",
    "class_stats = {\n",
    "    'epoch': 0,\n",
    "    'trainLoss': [],\n",
    "    'valAcc': [],\n",
    "    'valLoss': [],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_EPOCHS = 30\n",
    "train_classifier(\n",
    "    model,\n",
    "    classifier,\n",
    "    NUM_EPOCHS,\n",
    "    train_dataset,\n",
    "    val_dataset,\n",
    "    class_stats,\n",
    "    # eval_every=10,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(class_stats['trainLoss'])\n",
    "plt.show()\n",
    "plt.plot(class_stats['valAcc'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
