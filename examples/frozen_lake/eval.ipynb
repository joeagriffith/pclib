{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import time\n",
    "from q_learning import QL\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from pclib.nn.models import FCClassifierInv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                           \r"
     ]
    }
   ],
   "source": [
    "env = gym.make('FrozenLake-v1', is_slippery=False)\n",
    "ql = QL(env, 0.001, 0.9, epsilon=0.1)\n",
    "ql.train(10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.42000000000000004\n"
     ]
    }
   ],
   "source": [
    "terminated, truncated = False, False\n",
    "state = env.reset()[0]\n",
    "total_reward = 0\n",
    "while not terminated and not truncated:\n",
    "    action = env.action_space.sample()\n",
    "    state, reward, terminated, truncated, _ = env.step(action)\n",
    "    total_reward += reward - 0.01\n",
    "    if terminated:\n",
    "        total_reward -= 0.2\n",
    "    state = state\n",
    "print(total_reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                             "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean reward: 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "# Evaluate\n",
    "total_rewards = []\n",
    "num = 10000\n",
    "loop = tqdm(range(num), leave=False, total=num)\n",
    "for i in loop:\n",
    "    loop.set_description(f\"Evaluating {i}/{num}\")\n",
    "    total_reward = 0.0\n",
    "    state = env.reset()[0]\n",
    "    terminated, truncated = False, False\n",
    "    while not terminated and not truncated:\n",
    "        action = ql.act(state)\n",
    "        state, reward, terminated, truncated, _ =env.step(action)\n",
    "        total_reward += reward - 0.01\n",
    "        if terminated:\n",
    "            total_reward -= 0.2\n",
    "    total_rewards.append(reward)\n",
    "\n",
    "print(f\"mean reward: {np.array(total_rewards).mean()}\")\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "model = FCClassifierInv(\n",
    "    input_size=16,\n",
    "    num_classes=4,\n",
    "    hidden_sizes=[],\n",
    "    bias=True,\n",
    "    symmetric=True,\n",
    "    precision_weighted=False,\n",
    "    actv_fn=F.tanh,\n",
    "    steps=60,\n",
    "    gamma=0.1,\n",
    ").to(device)\n",
    "\n",
    "def format_obs(obs):\n",
    "    return F.one_hot(torch.tensor(obs), num_classes=16).float().to(device).unsqueeze(0)\n",
    "\n",
    "optimiser = torch.optim.AdamW(model.parameters(), lr=0.001, weight_decay=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                             \r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\joeag\\Documents\\pclib\\examples\\frozen_lake\\eval.ipynb Cell 7\u001b[0m line \u001b[0;36m1\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/joeag/Documents/pclib/examples/frozen_lake/eval.ipynb#X21sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mnot\u001b[39;00m terminated \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m truncated:\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/joeag/Documents/pclib/examples/frozen_lake/eval.ipynb#X21sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m     obs \u001b[39m=\u001b[39m format_obs(state)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/joeag/Documents/pclib/examples/frozen_lake/eval.ipynb#X21sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m     actions, s \u001b[39m=\u001b[39m model(obs)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/joeag/Documents/pclib/examples/frozen_lake/eval.ipynb#X21sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m     state, reward, terminated, truncated, _ \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39mstep(actions[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39margmax()\u001b[39m.\u001b[39mitem())\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/joeag/Documents/pclib/examples/frozen_lake/eval.ipynb#X21sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m     state_value \u001b[39m=\u001b[39m ql\u001b[39m.\u001b[39mQ[state]\u001b[39m.\u001b[39mmax()\n",
      "File \u001b[1;32mc:\\Users\\joeag\\Documents\\venvs\\ml-env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\joeag\\Documents\\pclib\\pclib\\nn\\models\\fc_classifier.py:247\u001b[0m, in \u001b[0;36mFCClassifier.forward\u001b[1;34m(self, obs, y, steps)\u001b[0m\n\u001b[0;32m    245\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(steps):\n\u001b[0;32m    246\u001b[0m     temp \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcalc_temp(i, steps)\n\u001b[1;32m--> 247\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstep(state, obs, y, temp)\n\u001b[0;32m    249\u001b[0m out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_output(state)\n\u001b[0;32m    251\u001b[0m \u001b[39mreturn\u001b[39;00m out, state\n",
      "File \u001b[1;32mc:\\Users\\joeag\\Documents\\pclib\\pclib\\nn\\models\\fc_classifier.py:142\u001b[0m, in \u001b[0;36mFCClassifier.step\u001b[1;34m(self, state, obs, y, temp)\u001b[0m\n\u001b[0;32m    140\u001b[0m     \u001b[39mfor\u001b[39;00m i, layer \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayers):\n\u001b[0;32m    141\u001b[0m         e_below \u001b[39m=\u001b[39m state[i\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m][\u001b[39m'\u001b[39m\u001b[39me\u001b[39m\u001b[39m'\u001b[39m] \u001b[39mif\u001b[39;00m i \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m--> 142\u001b[0m         layer\u001b[39m.\u001b[39mupdate_x(state[i], e_below)\n\u001b[0;32m    144\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpin(state, obs, y)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "num_epochs = 1000\n",
    "final_rewards = []\n",
    "loop = tqdm(range(num_epochs), leave=False, total=num_epochs)\n",
    "for epoch in loop:\n",
    "    if epoch > 0:\n",
    "        loop.set_description(f\"Epoch {epoch}/{num_epochs}, Reward: {np.array(final_rewards[-10:]).mean()}\")\n",
    "    state = env.reset()[0]\n",
    "    total_reward = 0.0\n",
    "    terminated, truncated = False, False\n",
    "    prev_state_value = ql.Q[state].max()\n",
    "    while not terminated and not truncated:\n",
    "\n",
    "        obs = format_obs(state)\n",
    "        actions, s = model(obs)\n",
    "\n",
    "        state, reward, terminated, truncated, _ = env.step(actions[0].argmax().item())\n",
    "        state_value = ql.Q[state].max()\n",
    "        total_reward += reward - 0.01\n",
    "        if terminated:\n",
    "            total_reward -= 0.2\n",
    "\n",
    "        loss = model.vfe(s) * (state_value - prev_state_value)\n",
    "        optimiser.zero_grad()\n",
    "        loss.backward()\n",
    "        optimiser.step()\n",
    "\n",
    "        prev_state_value = state_value\n",
    "    final_rewards.append(total_reward)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\joeag\\Documents\\pclib\\examples\\frozen_lake\\eval.ipynb Cell 5\u001b[0m line \u001b[0;36m1\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/joeag/Documents/pclib/examples/frozen_lake/eval.ipynb#W4sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m     state, reward, terminated, truncated, _ \u001b[39m=\u001b[39menv\u001b[39m.\u001b[39mstep(action)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/joeag/Documents/pclib/examples/frozen_lake/eval.ipynb#W4sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m     total_reward \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m reward\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/joeag/Documents/pclib/examples/frozen_lake/eval.ipynb#W4sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m     env\u001b[39m.\u001b[39;49mrender()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/joeag/Documents/pclib/examples/frozen_lake/eval.ipynb#W4sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m     time\u001b[39m.\u001b[39msleep(\u001b[39m0.1\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/joeag/Documents/pclib/examples/frozen_lake/eval.ipynb#W4sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m total_rewards\u001b[39m.\u001b[39mappend(reward)\n",
      "File \u001b[1;32mc:\\Users\\joeag\\Documents\\venvs\\ml-env\\Lib\\site-packages\\gymnasium\\core.py:471\u001b[0m, in \u001b[0;36mWrapper.render\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    469\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrender\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m RenderFrame \u001b[39m|\u001b[39m \u001b[39mlist\u001b[39m[RenderFrame] \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    470\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Uses the :meth:`render` of the :attr:`env` that can be overwritten to change the returned data.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 471\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv\u001b[39m.\u001b[39;49mrender()\n",
      "File \u001b[1;32mc:\\Users\\joeag\\Documents\\venvs\\ml-env\\Lib\\site-packages\\gymnasium\\wrappers\\order_enforcing.py:70\u001b[0m, in \u001b[0;36mOrderEnforcing.render\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     65\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_disable_render_order_enforcing \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_has_reset:\n\u001b[0;32m     66\u001b[0m     \u001b[39mraise\u001b[39;00m ResetNeeded(\n\u001b[0;32m     67\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mCannot call `env.render()` before calling `env.reset()`, if this is a intended action, \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m     68\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mset `disable_render_order_enforcing=True` on the OrderEnforcer wrapper.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m     69\u001b[0m     )\n\u001b[1;32m---> 70\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv\u001b[39m.\u001b[39;49mrender(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\joeag\\Documents\\venvs\\ml-env\\Lib\\site-packages\\gymnasium\\wrappers\\env_checker.py:67\u001b[0m, in \u001b[0;36mPassiveEnvChecker.render\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     65\u001b[0m     \u001b[39mreturn\u001b[39;00m env_render_passive_checker(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39menv, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m     66\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m---> 67\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv\u001b[39m.\u001b[39;49mrender(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\joeag\\Documents\\venvs\\ml-env\\Lib\\site-packages\\gymnasium\\envs\\toy_text\\frozen_lake.py:338\u001b[0m, in \u001b[0;36mFrozenLakeEnv.render\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    336\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_render_text()\n\u001b[0;32m    337\u001b[0m \u001b[39melse\u001b[39;00m:  \u001b[39m# self.render_mode in {\"human\", \"rgb_array\"}:\u001b[39;00m\n\u001b[1;32m--> 338\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_render_gui(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrender_mode)\n",
      "File \u001b[1;32mc:\\Users\\joeag\\Documents\\venvs\\ml-env\\Lib\\site-packages\\gymnasium\\envs\\toy_text\\frozen_lake.py:432\u001b[0m, in \u001b[0;36mFrozenLakeEnv._render_gui\u001b[1;34m(self, mode)\u001b[0m\n\u001b[0;32m    430\u001b[0m     pygame\u001b[39m.\u001b[39mevent\u001b[39m.\u001b[39mpump()\n\u001b[0;32m    431\u001b[0m     pygame\u001b[39m.\u001b[39mdisplay\u001b[39m.\u001b[39mupdate()\n\u001b[1;32m--> 432\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mclock\u001b[39m.\u001b[39;49mtick(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmetadata[\u001b[39m\"\u001b[39;49m\u001b[39mrender_fps\u001b[39;49m\u001b[39m\"\u001b[39;49m])\n\u001b[0;32m    433\u001b[0m \u001b[39melif\u001b[39;00m mode \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mrgb_array\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m    434\u001b[0m     \u001b[39mreturn\u001b[39;00m np\u001b[39m.\u001b[39mtranspose(\n\u001b[0;32m    435\u001b[0m         np\u001b[39m.\u001b[39marray(pygame\u001b[39m.\u001b[39msurfarray\u001b[39m.\u001b[39mpixels3d(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mwindow_surface)), axes\u001b[39m=\u001b[39m(\u001b[39m1\u001b[39m, \u001b[39m0\u001b[39m, \u001b[39m2\u001b[39m)\n\u001b[0;32m    436\u001b[0m     )\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Watch the agent play\n",
    "env = gym.make('FrozenLake-v1', is_slippery=False, render_mode='human')\n",
    "QL.env = env\n",
    "total_rewards = []\n",
    "for i in range(3):\n",
    "    total_reward = 0.0\n",
    "    state = env.reset()[0]\n",
    "    terminated, truncated = False, False\n",
    "    while not terminated and not truncated:\n",
    "        action = ql.act(state)\n",
    "        state, reward, terminated, truncated, _ =env.step(action)\n",
    "        total_reward += reward\n",
    "        env.render()\n",
    "        time.sleep(0.05)\n",
    "    total_rewards.append(reward)\n",
    "\n",
    "print(f\"mean reward: {np.array(total_rewards).mean()}\")\n",
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
